---
title: "Group A, in-lab"
output: pdf_document
---
<!-- # Setup -->
```{r E1-JH-setup, include = FALSE, echo = FALSE}
library(dplyr)
library(tidyr)
library(jsonlite)
library(ggplot2)
library(readr)
library(afex)
library(forcats)
library(papaja)
library(patchwork)
source('round.comm.R')

knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

<!-- # Import Data -->

<!-- Read JSON files and bind together into a tibble. -->

```{r import-data-JH}
jh.data.files <- list.files('data/data-in-lab-hartshorne', full.names = TRUE, pattern=".json")
jh.data.tables <- lapply(jh.data.files, function(file){
  data.table <- fromJSON(file)
  return(data.table)
})
jh.all.data <- bind_rows(jh.data.tables)
```

<!-- Mutate value types as needed -->

```{r value-types-JH}
jh.all.data <- jh.all.data %>%
  mutate(subject_id = factor(subject_id))
```

<!-- Get a list of subject IDs. -->

```{r Get subj IDs-JH}
jh.subjects <- unique(jh.all.data$subject_id)
```

<!-- # Extract Relevant Data -->

<!-- Get data from critical trials, select relevant columns, unnest/unpack webgazer data to get a tidy tibble, -->
<!-- and add gaze measurements as a percentage. -->

```{r Get data-JH}
jh.task.data <- jh.all.data %>%
  filter(verb_type %in% c("restricting", "non-restricting")) %>%
  select(subject_id, stimulus, response, verb_type, webgazer_data, webgazer_targets) %>%
  tidyr::unpack(webgazer_targets) %>%
  tidyr::unpack(`#scenes`) %>%
  select(-x,-y) %>%
  unnest(webgazer_data) %>%
  mutate(gaze.x.percent = (x - left) / width * 100) %>%
  mutate(gaze.y.percent = (y - top) / height * 100)
```

<!-- # Add Stimulus Info -->

<!-- Merge in info on visual ROIs and audio onsets. -->

```{r Merge stim info-JH, message=FALSE, warning=FALSE, include=FALSE}
#AJ note: I'm assuming it's the exact same stimuli, so using the same info files
ref.image.height <- 1500 # height of images that were measured for ROIs
ref.image.width <- 1875 # width of images that were measured for ROIs
padding <- 20 # padding in pixels around the ROIs.

image.data <- read_csv('info/image_measurements.csv') %>%
  mutate(roi_top = (top_object_distance - padding) / ref.image.height * 100,
         roi_bottom = (bottom_object_distance + padding) / ref.image.height * 100,
         roi_left = (left_object_distance - padding) / ref.image.width * 100,
         roi_right = (right_object_distance + padding) / ref.image.width * 100) %>%
  mutate(object_class = if_else(object_type %in% c('subject', 'target.object'), object_type, 'distractor')) %>%
  select(image_name, object_type, object_class, roi_top, roi_bottom, roi_left, roi_right)

audio.data <- read_csv('info/audio_timestamps.csv') %>% filter(!is.na(stimulus))

jh.critical.trials <- jh.task.data %>%
  left_join(audio.data) %>%
  left_join(image.data) 

#check for weirdness
discrep <- audio.data$delay_bw_onset_of_verb_and_onset_of_target_noun - 
  (audio.data$duration_of_verb + 
     audio.data$duration_of_intervening_determiner + 
     audio.data$intonation_break_bw_verb_and_determiner)
sum(discrep) 
#this should be zero, but it's 0.59! TO-DO: Check out the 3rd (audio/2r.mp3) and 22nd item (audio/11nr.mp3)
```

<!--Figure out word durations by condition (AK Table 2)-->
```{r Word durations-JH, message=FALSE, warning=FALSE, include=FALSE}
#First, grab the columns that match stimulus to condition name
jh.stim.cond <- distinct(data.frame(jh.critical.trials$stimulus, jh.critical.trials$verb_type))
colnames(jh.stim.cond) <- c("stimulus", "verb_type")

#Then merge with audio data
jh.word.dur.data <- left_join(audio.data, jh.stim.cond)

#Then summarize by condition
#word.dur.avg <- round.comm(colMeans(word.dur.data[,2:10])*1000)
jh.word.dur.avg <- jh.word.dur.data %>%
  group_by(verb_type) %>%
  summarise(Verb = round.comm(mean(duration_of_verb)*1000),
            Post_verbal_break = round.comm(mean(intonation_break_bw_verb_and_determiner)*1000),
            Determiner = round.comm(mean(duration_of_intervening_determiner)*1000),
            Total = Verb + Post_verbal_break + Determiner)

```

<!-- # Compute Gaze in ROI -->

<!-- Define a function for determining if `x,y` value falls in box. -->

```{r Gaze in ROI-JH}
in.box <- function(x, y, left, right, top, bottom, padding){
  is.in.the.box <- x >= left - padding & x <= right + padding & y >= top - padding & y <= bottom + padding
  return(is.in.the.box)
}
```

<!-- Add a column with boolean value for whether gaze was in the ROI. -->

```{r Yes/No in ROI-JH}
jh.critical.trials <- jh.critical.trials %>%
  mutate(is.fixating = in.box(gaze.x.percent, gaze.y.percent, roi_left, roi_right, roi_top, roi_bottom, 0))
```

<!-- # Subject Exclusion Criteria -->

<!-- We'll try this two ways. First if we adopt minimal exclusion criteria, removing subjects whose eye tracking data is essentially unusable. Second if we adopt aggressive exclusion criteria, removing subjects with obviously worse eye tracking data. -->

<!--###MINIMAL EXCLUSION-->
<!-- Set which mode here: -->
```{r Exclusion mode-JH}
#AJ note: we're just gonna do minimal
# 'AGGRESSIVE' or 'MINIMAL'
#exclusion.mode <- 'MINIMAL' 
```

<!-- ## Proportion of measured data in an ROI. -->

<!-- Check the proportion of samples that were in at least one ROI for subjects. 
Which is to say: at each event time "t" per trial per subject, check whether they were looking at ANY of the objects (vs background/off-screen). What proportion of fixation events (total number varies by trial/subj) were in ANY ROI? -->

```{r prop-in-ROI-JH, message=FALSE, warning=FALSE, include=FALSE}
jh.prop.in.roi.by.subject <- jh.critical.trials %>%
  group_by(subject_id, t, image_name) %>%
  summarize(fixation.in.roi = any(is.fixating)) %>%
  group_by(subject_id) %>%
  summarize(prop.of.samples.in.roi = mean(fixation.in.roi))
```
<!-- Visualize proportion of samples in ROI by subject. -->

```{r roi histogram-JH, eval=F}
#printed?
hist(jh.prop.in.roi.by.subject$prop.of.samples.in.roi, breaks=20)
```


<!-- Exclude subjects below a threshold -->

```{r exclude by prop in roi-JH}
prop.samples.threshold.min <- 0


jh.bad.subjects.prop.samples.min <- jh.prop.in.roi.by.subject %>%
  filter(prop.of.samples.in.roi <= prop.samples.threshold.min) %>%
  pull(subject_id) %>%
  as.character()
```

<!-- ## Validation accuracy -->

```{r get validation data-JH}
jh.validation.accuracy.data <- jh.all.data %>%
  filter(trial_type == 'webgazer-validate')
```

<!-- What % of subjects needed to recalibrate because of poor initial calibration? -->

```{r summarize calibration data-JH, eval=F}
#printed?
jh.validation.accuracy.data %>% 
  group_by(subject_id) %>%
  count() %>% 
  group_by(n) %>%
  count()
#AJ note: How many times does each P appear (n is the different values this can take: once, twice), and how many Ps appear at each of those (n) frequencies? 12 Ps appear once, 37 Ps appear twice. 
```

<!-- Get final average distance for each subject -->

```{r save validation percent in roi during validation-JH}
jh.validation.percent <- jh.validation.accuracy.data %>%
  group_by(subject_id) %>%
  slice_tail() %>%
  select(subject_id, percent_in_roi) %>%
  unnest(percent_in_roi) %>%
  group_by(subject_id) %>%
  summarise(M = mean(percent_in_roi), MAX = max(percent_in_roi), MIN = min(percent_in_roi)) %>%
  mutate(subject_id = fct_reorder(subject_id, M))

save(jh.validation.percent, file = "jh.validation.percent.RData")
```

```{r plot subjects by validation-JH, eval=F}
#printed?
ggplot(jh.validation.percent, aes(x=subject_id, y=M, ymax = MAX, ymin=MIN))+
  geom_pointrange() +
  theme_classic()
```
<!-- Set criteria for minimal exclusions (aggressive is later) -->

```{r filter by validation-JH}
validation.threshold.min <- 10

jh.bad.subjects.validation.min <- jh.validation.percent %>%
  filter(M <= validation.threshold.min) %>%
  pull(subject_id) %>%
  as.character()
```


<!-- ## Final exclusion data -->

```{r summarize exclusions-JH}
jh.bad.subjects.min <- unique(c(jh.bad.subjects.prop.samples.min, jh.bad.subjects.validation.min))

jh.critical.trials.filtered.min <- jh.critical.trials %>%
  filter(!subject_id %in% jh.bad.subjects.min)

n.jh.bad.subjects.min <- length(jh.bad.subjects.min)
#It's 2 (from the validation criterion)

```

#### Minimal Exclusion
As in the remote sample, we checked whether there were participants with `r prop.samples.threshold.min*100` percent of fixations in any ROIs and there were none. We then excluded participants with validation accuracy under `r validation.threshold.min` percent, resulting in `r n.jh.bad.subjects.min - length(jh.bad.subjects.prop.samples.min)` excluded participants. The following analyses included `r nlevels(droplevels(jh.critical.trials.filtered.min$subject_id))` participants. 

<!-- # Calculating cumulative fixation probability -->

<!-- We start by calculating cumulative fixation probability on the objects after the verb onset. (Any fixations prior to the verb onset are not relevant to the core research questions.) -->


<!-- First, add a column for time relative to verb onset, bin this to 50ms chunks, and filter the data to the 2,000 ms window after verb onset. -->

```{r set up 50 ms bins-JH, message=FALSE, warning=FALSE}
jh.cumulative.fixation.data.min <- jh.critical.trials.filtered.min %>%
  mutate(t.relative.to.verb.onset = t - (verb_onset*1000)) %>%
  filter(t.relative.to.verb.onset >= 0 & t.relative.to.verb.onset <2000) %>%
  mutate(t.w = floor(t.relative.to.verb.onset/50)*50)
```

<!-- Now check every time window within a trial to account for sparse samples. This calculating is time consuming, so we cache the result in a CSV file, and load from the CSV if it already exists. -->

```{r slowly build cumu fix data-JH, message=FALSE, warning=FALSE}
cumulative.fixation.calculation <- function(df){
  time <- seq(0,1950,50)
  fixations <- logical(length(time))
  hit.flag <- FALSE
  for(i in 1:length(fixations)){
    if(hit.flag){
      fixations[i] <- TRUE
    } else {
      val <- df %>% filter(t.w <= time[i]) %>% pull(is.fixating) %>% any()
      if(val == TRUE) {
        hit.flag <- TRUE
      }
      fixations[i] <- val
    }
  }
  out <- tibble(t.window = time, has.fixated = fixations)
  return(out)
}

if(file.exists("data/data-in-lab-hartshorne/generated/jh.cumulative_fixation_data_min.csv")){
  jh.cumulative.fixation.windows.data.min <- read_csv("data/data-in-lab-hartshorne/generated/jh.cumulative_fixation_data_min.csv")
} else {

  jh.cumulative.fixation.windows.data.min <- jh.cumulative.fixation.data.min %>%
    group_by(subject_id, verb_type, image_name, object_type, object_class) %>%
    summarize(cumulative.fixation.calculation(cur_data()))
  
  write_csv(jh.cumulative.fixation.windows.data.min, file="data/data-in-lab-hartshorne/generated/jh.cumulative_fixation_data_min.csv")
}
```

<!-- Then group by object class to calculate fixation probability for each kind of object, normalizing by the number of objects of that type. -->

```{r group cumu fix by object-JH, message=FALSE, warning=FALSE}
jh.cumulative.fixation.windows.grouped.data.min <- jh.cumulative.fixation.windows.data.min %>%
  filter(!subject_id %in% jh.bad.subjects.min) %>%
  group_by(subject_id, verb_type, image_name, object_class, t.window) %>%
  summarize(cumulative.fixation.p = mean(has.fixated)) 
```

<!-- Then collapse over trials to get a cumulative probability for the whole window for each subject -->

```{r collapse cumu fix over trials-JH, message=FALSE, warning=FALSE}
jh.cumulative.fixation.windows.grouped.average.data.min <- jh.cumulative.fixation.windows.grouped.data.min %>%
  group_by(subject_id, verb_type, object_class, t.window) %>%
  summarize(cumulative.fixation.p = mean(cumulative.fixation.p))
```

<!-- Collapse across subjects to generate the equivalent of the figure from Altmann & Kamide. -->

```{r create summary data for spag plot-JH, message=FALSE, warning=FALSE}
jh.cumulative.fixation.data.summary.min <- jh.cumulative.fixation.windows.grouped.average.data.min %>%
  filter(object_class %in% c('target.object', 'distractor')) %>%
  group_by(t.window, verb_type, object_class) %>%
  summarize(cumulative.fixation.m = mean(cumulative.fixation.p))
```

#### Cumulative Fixation Probabilities
For each sentence, the target time window began at the onset of the verb and ended 2000 milliseconds later. This window was then divided into 50-ms bins; for each participant and each trial, we recorded whether each object was fixated during the 50-ms bin. Collapsing over trials and participants, and averaging across distractors, we calculated the cumulative probability of fixation, shown in Figure \@ref(fig:jh-E1-spaghetti-fig).

```{r jh-E1-spaghetti-fig, fig.cap = "Cumulative probability of fixating distractor and target objects across conditions over time, with 0 ms aligned to the verb onset time. The vertical line marks the mean noun onset time across trials and conditions."} 
jh.E1_spag_fig <- ggplot(jh.cumulative.fixation.data.summary.min, aes(x=t.window, y=cumulative.fixation.m, fill=object_class, shape=verb_type, group=interaction(verb_type, object_class)))+
  scale_fill_grey(start=1.0, end=0.0)+
  scale_shape_manual(values=c(22,21))+
  geom_line()+
  geom_point(size=2)+
  theme_bw()+
  theme(panel.grid=element_blank())+
  scale_y_continuous(limits=c(0,1))+
  scale_x_continuous(expand=c(0.01,0.01))+
  labs(x="Time from Verb Onset (ms)", y="Probability", shape="Verb Type", fill="Object Type")+
  geom_vline(xintercept = mean(jh.word.dur.avg$Total))

jh.E1_spag_fig

#The "Object type" part of the key doesn't actually show which color is which
```

 

<!-- __Question 1: Are there more pre-noun first looks to the target object than to the distractors?__

First, grab the noun onset times to merge into our cumulative fixation data.-->

```{r set up noun onset times-JH}
jh.noun.onset.times.min <- jh.critical.trials.filtered.min %>%
  group_by(image_name, verb_type) %>%
  slice(1) %>%
  ungroup() %>%
  select(image_name, verb_type, delay_bw_onset_of_verb_and_onset_of_target_noun) %>%
  mutate(relative.noun.onset = delay_bw_onset_of_verb_and_onset_of_target_noun * 1000) %>%
  select(-delay_bw_onset_of_verb_and_onset_of_target_noun)

jh.trial.level.cumulative.fixation.data.min <- jh.cumulative.fixation.windows.grouped.data.min %>%
  left_join(jh.noun.onset.times.min, by=c("image_name", "verb_type"))
```

<!--Then, grab the last time window before the noun onset for each trial. This is the critical data for this test.-->

```{r find prenoun window data-JH}
jh.pre.noun.first.looks.data.min <- jh.trial.level.cumulative.fixation.data.min %>%
  group_by(subject_id, image_name, verb_type) %>%
  filter(t.window < relative.noun.onset) %>%
  filter(t.window == max(t.window)) %>%
  filter(object_class %in% c("target.object", "distractor"))
```

```{r save prenoun window data-JH}
jh.pre.noun.first.looks.data.min$verb_type <- as.factor(jh.pre.noun.first.looks.data.min$verb_type)
jh.pre.noun.first.looks.data.min$object_class <- as.factor(jh.pre.noun.first.looks.data.min$object_class)

save(jh.pre.noun.first.looks.data.min, file = "jh.pre.noun.first.looks.data.min.RData")
```
<!--Fit the model, with no covariance between random effects because the model cannot converge with full covariance matrix.-->

```{r run model 1-JH, message=FALSE, warning=FALSE}
jh.E1_model1.min <- afex::lmer_alt(cumulative.fixation.p ~ object_class * verb_type + (object_class * verb_type || image_name) + (object_class * verb_type || subject_id) , data=jh.pre.noun.first.looks.data.min)
#summary(jh.E1_model1.min)

jh.E1_model1_tab.min = broom.mixed::tidy(jh.E1_model1.min) 
jh.E1_model1_obj.min = jh.E1_model1_tab.min %>% filter(term == "object_classtarget.object")
jh.E1_model1_verb.min = jh.E1_model1_tab.min %>% filter(term == "verb_typerestricting")
jh.E1_model1_obj_verb.min = jh.E1_model1_tab.min %>% filter(term == "object_classtarget.object:verb_typerestricting")
```
#### Pre-noun fixations 
<!--AJ: Edit text -->
In our first two analyses, we ask whether participants looked more to the target than to the distractor during the predictive time window, given that the verb is restricting. The first model tested whether there were more fixations to the target object than to the distractor in the time window before the onset of the target noun. We ran a regression model predicting the cumulative fixation probability in the last 50-ms bin before noun onset from the verb condition (restricting = 1 vs. non-restricting = 0), object type (target = 1 vs. distractor = 0), and their interaction, along with random effects for participants and images (with no covariance between random effects because the model cannot converge with full covariance matrix). There were no significant effects, although the critical interaction was in the right direction [bar graph?] (_b_ = `r jh.E1_model1_obj_verb.min %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r jh.E1_model1_obj_verb.min %>% pull(std.error) %>% round(digits = 2)`, _p_=`r jh.E1_model1_obj_verb.min %>% pull(p.value) %>% round(digits = 2)`). 


<!--Verb offset times-->

```{r find verb offset data-JH}
jh.verb.offset.times.min <- jh.critical.trials.filtered.min %>%
  group_by(image_name, verb_type) %>%
  slice(1) %>%
  ungroup() %>%
  select(image_name, verb_type, duration_of_verb) %>%
  mutate(relative.verb.offset = duration_of_verb * 1000) %>%
  select(-duration_of_verb)

jh.trial.level.cumulative.fixation.data.verb.offset.min <- jh.cumulative.fixation.windows.grouped.data.min %>%
  left_join(jh.verb.offset.times.min, by=c("image_name", "verb_type"))
```

```{r save verb offset data-JH}
jh.pre.verb.offset.first.looks.data.min <- jh.trial.level.cumulative.fixation.data.verb.offset.min %>%
  group_by(subject_id, image_name, verb_type) %>%
  filter(t.window < relative.verb.offset) %>%
  filter(t.window == max(t.window)) %>%
  filter(object_class %in% c("target.object", "distractor"))

save(jh.pre.verb.offset.first.looks.data.min, file = "jh.pre.verb.offset.first.looks.data.min.RData")
```


```{r run model 2-JH, message=FALSE, warning=FALSE}
#model <- lmer_alt(cumulative.fixation.p ~ object_class * verb_type + (object_class * verb_type || image_name) + (object_class * verb_type || subject_id) , data=jh.pre.verb.offset.first.looks.data.min)
#summary(model)

jh.E1_model2.min <- afex::lmer_alt(cumulative.fixation.p ~ object_class * verb_type + (object_class * verb_type || image_name) + (object_class * verb_type || subject_id) , data=jh.pre.verb.offset.first.looks.data.min)
#summary(jh.E1_model2.min)

jh.E1_model2_tab.min = broom.mixed::tidy(jh.E1_model2.min) 
jh.E1_model2_obj.min = jh.E1_model2_tab.min %>% filter(term == "object_classtarget.object")
jh.E1_model2_verb.min = jh.E1_model2_tab.min %>% filter(term == "verb_typerestricting")
jh.E1_model2_obj_verb.min = jh.E1_model2_tab.min %>% filter(term == "object_classtarget.object:verb_typerestricting")
```
#### Pre-verb-offset fixations
Altmann & Kamide tested a second model, aligning the predictive time window with the offset of the verb rather than the onset of the noun as above. When we do the same, we again see that the critical interaction is not significant but numerically in the expected direction (_b_ = `r jh.E1_model2_obj_verb.min %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r jh.E1_model2_obj_verb.min %>% pull(std.error) %>% round(digits = 2)`, _p_=`r jh.E1_model2_obj_verb.min %>% pull(p.value) %>% round(digits = 2)`).

<!--Relative to noun onset, when is the first (post-verb) fixation on the target?-->

```{r}
jh.first.fixation.after.verb.onset.min <- jh.critical.trials.filtered.min %>%
  group_by(subject_id, image_name) %>%
  filter(t >= verb_onset*1000) %>%
  filter(object_type == "target.object") %>%
  filter(is.fixating == TRUE) %>%
  filter(t == min(t)) %>%
  mutate(noun_relative.t = t - (postverbal_noun_onset*1000)) %>%
  mutate(von_relative.t = t - (verb_onset*1000)) %>%
  mutate(voff_relative.t = t - (verb_offset*1000)) %>%
  mutate(det_relative.t = t -(postverbal_determiner_onset*1000))
```

```{r E1-latency-table-min-JH, echo=FALSE, eval=FALSE}
#summarize data for table
jh.first.fix.avg.min <- jh.first.fixation.after.verb.onset.min %>%
  group_by(verb_type) %>%
  summarise(Verb.on = round.comm(mean(von_relative.t)),
            Verb.off = round.comm(mean(voff_relative.t)),
            Det.on = round.comm(mean(det_relative.t)),
            Noun.on = round.comm(mean(noun_relative.t)))

#put data into a data frame
jh.our_table1_min <- data.frame(
  Time = c("Verb onset", "Verb offset", "Determiner onset", "Noun onset"),
  Eat = c(jh.first.fix.avg.min$Verb.on[2],
           jh.first.fix.avg.min$Verb.off[2],
           jh.first.fix.avg.min$Det.on[2],
           jh.first.fix.avg.min$Noun.on[2]),
  Move = c(jh.first.fix.avg.min$Verb.on[1],
           jh.first.fix.avg.min$Verb.off[1],
           jh.first.fix.avg.min$Det.on[1],
           jh.first.fix.avg.min$Noun.on[1]),
  Difference = c(jh.first.fix.avg.min$Verb.on[1]-jh.first.fix.avg.min$Verb.on[2],
                  jh.first.fix.avg.min$Verb.off[1]-jh.first.fix.avg.min$Verb.off[2],
                  jh.first.fix.avg.min$Det.on[1]-jh.first.fix.avg.min$Det.on[2],
                  jh.first.fix.avg.min$Noun.on[1]-jh.first.fix.avg.min$Noun.on[2])
)

# AK_table1 <- data.frame(
#   Time = c("Verb onset", "Verb offset", "Determiner onset", "Noun onset"),
#   Eat = c(611, 228, 37, -85),
#   Move = c(838, 415, 234, 127),
#   Difference = c(227, 187, 197, 212))
# 
# E1_latency_table_min <- merge(our_table1_min, AK_table1, by = "Time")
#   
# apa_table(
#   E1_latency_table_min,
#   caption = "Onset of first post-verb-onset saccade, relative to verb onset, verb offset, determiner onset, and noun onset, in ms.",
#   font_size = "small",
#    col_spanners = list(`Current Exp. 1` = c(2, 4), `Altmann $ Kamide Exp. 1` = c(5, 7))
# )
# 
# 
# #can I add a third dimension i.e. have two headings to group our data vs. AK data?
# 
# #knitr::kable(table1_min)
```

```{r}
jh.first.fixation.after.verb.onset.analysis.data.min <- jh.first.fixation.after.verb.onset.min %>%
  ungroup() %>%
  select(subject_id, image_name, verb_type, duration_of_verb, noun_relative.t) %>%
  mutate(dur_verb_ms_c = scale(duration_of_verb, scale = FALSE)*1000) #mean-center verb duration

save(jh.first.fixation.after.verb.onset.analysis.data.min, file = "jh.first.fixation.after.verb.onset.analysis.data.min.RData")
```


```{r run model 3-JH, message=FALSE, warning=FALSE}
jh.E1_model3.min <- lmer_alt(noun_relative.t ~ verb_type*dur_verb_ms_c + (verb_type || subject_id) + (verb_type || image_name), data=jh.first.fixation.after.verb.onset.analysis.data.min)

#summary(jh.E1_model3.min)

jh.E1_model3_tab.min = broom.mixed::tidy(jh.E1_model3.min) 
jh.E1_model3_verb.min = jh.E1_model3_tab.min %>% filter(term == "verb_typerestricting")

```
#### First target fixations after verb
Finally, we address whether participants look to the target faster in the restrictive vs. the non-restrictive condition, starting after the onset of the verb. [TO-DO: On average, participants looked to the target X ms after (AK's Table 1)..., I'll also want to say the lengths of the verbs. AK's Table 2] We ran a regression model predicting the timing of the first fixation to the target object, relative to the onset of the noun, with verb condition as a predictor, mean-centered verb duration as a covariate, and random intercepts and condition slopes for participants and scenes. There were no significant effects; participants looked sooner at the target in the restrictive condition, while accounting for verb duration and its interaction with condition, but this was not a statistically significant effect (_b_ = `r jh.E1_model3_verb.min %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r jh.E1_model3_verb.min %>% pull(std.error) %>% round(digits = 2)`, _p_=`r jh.E1_model3_verb.min %>% pull(p.value) %>% round(digits = 2)`).

```{r}
jh.first.fixation.after.verb.onset.subject.summary.min <- jh.first.fixation.after.verb.onset.min %>%
  group_by(subject_id, verb_type) %>%
  summarize(m.t = mean(noun_relative.t), ct = n())

jh.first.fixation.after.verb.onset.summary.min <- jh.first.fixation.after.verb.onset.subject.summary.min %>%
  group_by(verb_type) %>%
  summarize(m=mean(m.t), sd=sd(m.t), se=sd/sqrt(n()))

save(jh.first.fixation.after.verb.onset.summary.min, file = "jh.first.fixation.after.verb.onset.subject.summary.min.RData")
```


```{r}
jh.E1_first_fix_plot <- ggplot(jh.first.fixation.after.verb.onset.summary.min, aes(x=verb_type, y=m, ymin=m-se,ymax=m+se))+
  geom_col()+
  geom_errorbar(width=0.2)
```

<!-- CALIBRATION ANALYSIS -->

```{r rename stuff-JH}
jh.pre.noun.first.looks.data <- jh.pre.noun.first.looks.data.min

jh.pre.verb.offset.first.looks.data <- jh.pre.verb.offset.first.looks.data.min

jh.first.fixation.after.verb.onset.analysis.data <- jh.first.fixation.after.verb.onset.analysis.data.min

jh.first.fixation.after.verb.onset.subject.summary <- jh.first.fixation.after.verb.onset.subject.summary.min  
```

```{r Model 1 subject summary-JH}
jh.pre.noun.subj.summary <- jh.pre.noun.first.looks.data %>%
  group_by(subject_id, verb_type, object_class) %>%
  summarize(fix.p = mean(cumulative.fixation.p))

jh.pre.noun.by.verb <- jh.pre.noun.subj.summary %>%
  pivot_wider(names_from = object_class, values_from = fix.p) %>%
  mutate(tar.adv = target.object - distractor)

jh.pre.noun.effects <- jh.pre.noun.by.verb %>%
  select(subject_id, verb_type, tar.adv) %>%
  pivot_wider(names_from = verb_type, values_from = tar.adv) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(effect = restricting - non_restricting)
```

```{r Model 1 effect size by calibration-JH}
#merge with validation percent

jh.cali.analy.1 <- merge(jh.pre.noun.effects, jh.validation.percent, by = "subject_id")

#correlate
jh.m1_diffdiff_cor <- cor.test(jh.cali.analy.1$effect, jh.cali.analy.1$M, use = "pairwise.complete.obs") #difference of differences
#0.03107354
jh.m1_diffdiff_cor_r <- jh.m1_diffdiff_cor$estimate
jh.m1_diffdiff_cor_p <- jh.m1_diffdiff_cor$p.value

jh.m1_diff_cor <- cor.test(jh.cali.analy.1$restricting, jh.cali.analy.1$M, use = "pairwise.complete.obs") #target adv. in restricting condition only
#0.2096605
jh.m1_diff_cor_r <- jh.m1_diff_cor$estimate
jh.m1_diff_cor_p <- jh.m1_diff_cor$p.value
```

```{r Model 2 subject summary-JH}
jh.pre.verb.subj.summary <- jh.pre.verb.offset.first.looks.data %>%
  group_by(subject_id, verb_type, object_class) %>%
  summarize(fix.p = mean(cumulative.fixation.p))

jh.pre.verb.by.verb <- jh.pre.verb.subj.summary %>%
  pivot_wider(names_from = object_class, values_from = fix.p) %>%
  mutate(tar.adv = target.object - distractor)

jh.pre.verb.effects <- jh.pre.verb.by.verb %>%
  select(subject_id, verb_type, tar.adv) %>%
  pivot_wider(names_from = verb_type, values_from = tar.adv) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(effect = restricting - non_restricting)
```

```{r Model 2 effect size by calibration-JH}
#merge with validation percent
jh.cali.analy.2 <- merge(jh.pre.verb.effects, jh.validation.percent, by = "subject_id")

#correlate
jh.m2_diffdiff_cor <- cor.test(jh.cali.analy.2$effect, jh.cali.analy.2$M, use = "pairwise.complete.obs") #difference of differences
#-0.04809241
jh.m2_diffdiff_cor_r <- jh.m2_diffdiff_cor$estimate
jh.m2_diffdiff_cor_p <- jh.m2_diffdiff_cor$p.value

jh.m2_diff_cor <- cor.test(jh.cali.analy.2$restricting, jh.cali.analy.2$M, use = "pairwise.complete.obs") #target adv. in restricting condition only
#-0.03601876
jh.m2_diff_cor_r <- jh.m2_diff_cor$estimate
jh.m2_diff_cor_p <- jh.m2_diff_cor$p.value

```

```{r Model 3 effect size by calibration-JH}
#summary(validation.percent)
#make looking scores for Ps
jh.looking.scores <- jh.first.fixation.after.verb.onset.subject.summary[,1:3] %>% 
  spread(verb_type, m.t) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(diff = non_restricting - restricting)


#merge with validation percent
jh.cali.analy.3 <- merge(jh.looking.scores, jh.validation.percent, by = "subject_id")

#correlate
jh.m3_diff_cor <- cor.test(jh.cali.analy.3$diff, jh.cali.analy.3$M, use = "pairwise.complete.obs")
#0.04122796
jh.m3_diff_cor_r <- jh.m3_diff_cor$estimate
jh.m3_diff_cor_p <- jh.m3_diff_cor$p.value
```
#### Calibration
As before, participants' calibration quality was measured as the mean percentage of fixations that landed within 200 pixels of the calibration point. Calibration quality ranged from `r min(jh.validation.percent$M) %>% round(digits = 2)`% to `r max(jh.validation.percent$M) %>% round(digits = 2)`%. 

We tested whether a participant's calibration quality was correlated with their effect size. Across the three condition effects of interest, calibration quality was not significantly correlated (Effect 1 (pre-noun-onset): Pearson's _r_ = `r jh.m1_diffdiff_cor_r`, _p_ = `r jh.m1_diffdiff_cor_p`, Effect 2 (pre-verb-offset): Pearson's _r_ = `r jh.m2_diffdiff_cor_r`, _p_ = `r jh.m2_diffdiff_cor_p`, Effect 3 (first fixation): Pearson's _r_ = `r jh.m3_diff_cor_r`, _p_ = `r jh.m3_diff_cor_p`. However, when the two interaction effects are calculated as the target advantage in the restricting condition only (i.e. rather than a difference of differences), we see a significant correlation between target advantage and calibration quality in the wider pre-noun window (Pearson's _r_ = `r jh.m1_diff_cor_r`, _p_ = `r jh.m1_diff_cor_p`).