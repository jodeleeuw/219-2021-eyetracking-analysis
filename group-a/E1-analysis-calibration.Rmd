```{r}
#To be run after E1-analysis-replication.Rmd
#Correlates subjects' experimental effects with their calibration quality (from df called "validation.percent").
```
<!-- # Setup -->
```{r E1-c-setup, message=FALSE, warning=FALSE, include = FALSE, echo = FALSE}
# library(dplyr)
# library(tidyr)
# library(jsonlite)
# library(ggplot2)
# library(readr)
# library(afex)
# library(forcats)
# library(papaja)
# source("~/round.comm.R")

knitr::opts_chunk$set(include = FALSE, echo = FALSE)
#knitr::opts_knit$set(root.dir =)

#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

```{r Load data}
# load("pre.noun.first.looks.data.min.RData") 
# load("pre.verb.offset.first.looks.data.min.RData")
# load("first.fixation.after.verb.onset.analysis.data.min.RData")
# load("validation.percent.RData")
# load("first.fixation.after.verb.onset.subject.summary.min.RData")

pre.noun.first.looks.data <- pre.noun.first.looks.data.min
pre.verb.offset.first.looks.data <- pre.verb.offset.first.looks.data.min
first.fixation.after.verb.onset.analysis.data <- first.fixation.after.verb.onset.analysis.data.min
first.fixation.after.verb.onset.subject.summary <- first.fixation.after.verb.onset.subject.summary.min  
```

```{r Model 1 subject summary}
pre.noun.subj.summary <- pre.noun.first.looks.data %>%
  group_by(subject_id, verb_type, object_class) %>%
  summarize(fix.p = mean(cumulative.fixation.p))

pre.noun.by.verb <- pre.noun.subj.summary %>%
  pivot_wider(names_from = object_class, values_from = fix.p) %>%
  mutate(tar.adv = target.object - distractor)

pre.noun.effects <- pre.noun.by.verb %>%
  select(subject_id, verb_type, tar.adv) %>%
  pivot_wider(names_from = verb_type, values_from = tar.adv) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(effect = restricting - non_restricting)
```

```{r Model 1 effect size by calibration}
#merge with validation percent

cali.analy.1 <- merge(pre.noun.effects, validation.percent, by = "subject_id")

#correlate
m1_diffdiff_cor <- cor.test(cali.analy.1$effect, cali.analy.1$M, use = "pairwise.complete.obs") #difference of differences
#0.03107354
m1_diffdiff_cor_r <- m1_diffdiff_cor$estimate
m1_diffdiff_cor_p <- m1_diffdiff_cor$p.value

m1_diff_cor <- cor.test(cali.analy.1$restricting, cali.analy.1$M, use = "pairwise.complete.obs") #target adv. in restricting condition only
#0.2096605
m1_diff_cor_r <- m1_diff_cor$estimate
m1_diff_cor_p <- m1_diff_cor$p.value
```

```{r Model 2 subject summary}
pre.verb.subj.summary <- pre.verb.offset.first.looks.data %>%
  group_by(subject_id, verb_type, object_class) %>%
  summarize(fix.p = mean(cumulative.fixation.p))

pre.verb.by.verb <- pre.verb.subj.summary %>%
  pivot_wider(names_from = object_class, values_from = fix.p) %>%
  mutate(tar.adv = target.object - distractor)

pre.verb.effects <- pre.verb.by.verb %>%
  select(subject_id, verb_type, tar.adv) %>%
  pivot_wider(names_from = verb_type, values_from = tar.adv) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(effect = restricting - non_restricting)
```

```{r Model 2 effect size by calibration}
#merge with validation percent
cali.analy.2 <- merge(pre.verb.effects, validation.percent, by = "subject_id")

#correlate
m2_diffdiff_cor <- cor.test(cali.analy.2$effect, cali.analy.2$M, use = "pairwise.complete.obs") #difference of differences
#-0.04809241
m2_diffdiff_cor_r <- m2_diffdiff_cor$estimate
m2_diffdiff_cor_p <- m2_diffdiff_cor$p.value

m2_diff_cor <- cor.test(cali.analy.2$restricting, cali.analy.2$M, use = "pairwise.complete.obs") #target adv. in restricting condition only
#-0.03601876
m2_diff_cor_r <- m2_diff_cor$estimate
m2_diff_cor_p <- m2_diff_cor$p.value

```

```{r Model 3 effect size by calibration}
#summary(validation.percent)
#make looking scores for Ps
looking.scores <- first.fixation.after.verb.onset.subject.summary[,1:3] %>% 
  spread(verb_type, m.t) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(diff = non_restricting - restricting)


#merge with validation percent
cali.analy.3 <- merge(looking.scores, validation.percent, by = "subject_id")

#correlate
m3_diff_cor <- cor.test(cali.analy.3$diff, cali.analy.3$M, use = "pairwise.complete.obs")
#0.04122796
m3_diff_cor_r <- m3_diff_cor$estimate
m3_diff_cor_p <- m3_diff_cor$p.value
```

Participants' calibration quality was measured as the mean percentage of fixations that landed within 200 pixels of the calibration point. Calibration quality varied widely, ranging from `r min(validation.percent$M) %>% round(digits = 2)`% to `r max(validation.percent$M) %>% round(digits = 2)`%. 

We tested whether a participant's calibration quality was correlated with their effect size. The quality of a participant's calibration was not significantly correlated with the participant's effect size. There were three effects of interest: the verb-by-object interaction in predicting fixation probabilities, both in the (1) pre-noun-onset and (2) pre-verb-offset windows (calculated as the difference in target-over-distractor preference between verb conditions), and (3) the effect of verb on the timing of the first target fixation (calculated as the difference in target latency between verb conditions). Across the three effects of interest, calibration quality was not significantly correlated (Effect 1: Pearson's _r_ = `r m1_diffdiff_cor_r`, _p_ = `r m1_diffdiff_cor_p`, Effect 2: Pearson's _r_ = `r m2_diffdiff_cor_r`, _p_ = `r m2_diffdiff_cor_p`, Effect 3: Pearson's _r_ = `r m3_diff_cor_r`, _p_ = `r m3_diff_cor_p`. However, when the two interaction effects are calculated as the target advantage in the restricting condition only (i.e. rather than a difference of differences), we see a significant correlation between target advantage and calibration quality in the wider pre-noun window (Pearson's _r_ = `r m1_diff_cor_r`, _p_ = `r m1_diff_cor_p`).