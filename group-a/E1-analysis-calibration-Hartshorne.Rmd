---
title: "E1 JH data calibration"
author: "Ariel James"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r}
#To be run after E1-analysis-replication.Rmd
#Correlates subjects' experimental effects with their calibration quality (from df called "validation.percent").
```
<!-- # Setup -->
```{r E1-c-JH-setup, message=FALSE, warning=FALSE, include = FALSE, echo = FALSE}
library(dplyr)
library(tidyr)
library(jsonlite)
library(ggplot2)
library(readr)
library(afex)
library(forcats)
library(papaja)
source("~/round.comm.R")

knitr::opts_chunk$set(include = FALSE, echo = FALSE, message = FALSE, warning = FALSE)
#knitr::opts_knit$set(root.dir =)

#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

```{r Load data}
attach("jh.pre.noun.first.looks.data.min.RData")
attach("jh.pre.verb.offset.first.looks.data.min.RData")
attach("jh.first.fixation.after.verb.onset.analysis.data.min.RData")
attach("jh.validation.percent.RData")
#AJ: for some reason it can't find the next one:
attach("jh.first.fixation.after.verb.onset.subject.summary.min.RData")

jh.pre.noun.first.looks.data <- jh.pre.noun.first.looks.data.min

jh.pre.verb.offset.first.looks.data <- jh.pre.verb.offset.first.looks.data.min

jh.first.fixation.after.verb.onset.analysis.data <- jh.first.fixation.after.verb.onset.analysis.data.min

jh.first.fixation.after.verb.onset.subject.summary <- jh.first.fixation.after.verb.onset.subject.summary.min  
```

```{r Model 1 subject summary}
jh.pre.noun.subj.summary <- jh.pre.noun.first.looks.data %>%
  group_by(subject_id, verb_type, object_class) %>%
  summarize(fix.p = mean(cumulative.fixation.p))

jh.pre.noun.by.verb <- jh.pre.noun.subj.summary %>%
  pivot_wider(names_from = object_class, values_from = fix.p) %>%
  mutate(tar.adv = target.object - distractor)

jh.pre.noun.effects <- jh.pre.noun.by.verb %>%
  select(subject_id, verb_type, tar.adv) %>%
  pivot_wider(names_from = verb_type, values_from = tar.adv) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(effect = restricting - non_restricting)
```

```{r Model 1 effect size by calibration}
#merge with validation percent

jh.cali.analy.1 <- merge(jh.pre.noun.effects, jh.validation.percent, by = "subject_id")

#correlate
jh.m1_diffdiff_cor <- cor.test(jh.cali.analy.1$effect, jh.cali.analy.1$M, use = "pairwise.complete.obs") #difference of differences
#0.03107354
jh.m1_diffdiff_cor_r <- jh.m1_diffdiff_cor$estimate
jh.m1_diffdiff_cor_p <- jh.m1_diffdiff_cor$p.value

jh.m1_diff_cor <- cor.test(jh.cali.analy.1$restricting, jh.cali.analy.1$M, use = "pairwise.complete.obs") #target adv. in restricting condition only
#0.2096605
jh.m1_diff_cor_r <- jh.m1_diff_cor$estimate
jh.m1_diff_cor_p <- jh.m1_diff_cor$p.value
```

```{r Model 2 subject summary}
jh.pre.verb.subj.summary <- jh.pre.verb.offset.first.looks.data %>%
  group_by(subject_id, verb_type, object_class) %>%
  summarize(fix.p = mean(cumulative.fixation.p))

jh.pre.verb.by.verb <- jh.pre.verb.subj.summary %>%
  pivot_wider(names_from = object_class, values_from = fix.p) %>%
  mutate(tar.adv = target.object - distractor)

jh.pre.verb.effects <- jh.pre.verb.by.verb %>%
  select(subject_id, verb_type, tar.adv) %>%
  pivot_wider(names_from = verb_type, values_from = tar.adv) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(effect = restricting - non_restricting)
```

```{r Model 2 effect size by calibration}
#merge with validation percent
jh.cali.analy.2 <- merge(jh.pre.verb.effects, jh.validation.percent, by = "subject_id")

#correlate
jh.m2_diffdiff_cor <- cor.test(jh.cali.analy.2$effect, jh.cali.analy.2$M, use = "pairwise.complete.obs") #difference of differences
#-0.04809241
jh.m2_diffdiff_cor_r <- jh.m2_diffdiff_cor$estimate
jh.m2_diffdiff_cor_p <- jh.m2_diffdiff_cor$p.value

jh.m2_diff_cor <- cor.test(jh.cali.analy.2$restricting, jh.cali.analy.2$M, use = "pairwise.complete.obs") #target adv. in restricting condition only
#-0.03601876
jh.m2_diff_cor_r <- jh.m2_diff_cor$estimate
jh.m2_diff_cor_p <- jh.m2_diff_cor$p.value

```

```{r Model 3 effect size by calibration}
#summary(validation.percent)
#make looking scores for Ps
jh.looking.scores <- jh.first.fixation.after.verb.onset.subject.summary[,1:3] %>% 
  spread(verb_type, m.t) %>%
  rename("non_restricting" = "non-restricting") %>%
  mutate(diff = non_restricting - restricting)


#merge with validation percent
jh.cali.analy.3 <- merge(jh.looking.scores, jh.validation.percent, by = "subject_id")

#correlate
jh.m3_diff_cor <- cor.test(jh.cali.analy.3$diff, jh.cali.analy.3$M, use = "pairwise.complete.obs")
#0.04122796
jh.m3_diff_cor_r <- jh.m3_diff_cor$estimate
jh.m3_diff_cor_p <- jh.m3_diff_cor$p.value
```

As before, participants' calibration quality was measured as the mean percentage of fixations that landed within 200 pixels of the calibration point. Calibration quality ranged from `r min(jh.validation.percent$M) %>% round(digits = 2)`% to `r max(jh.validation.percent$M) %>% round(digits = 2)`%. 

We tested whether a participant's calibration quality was correlated with their effect size. Across the three condition effects of interest, calibration quality [was? was not?] significantly correlated (Effect 1 (pre-noun-onset): Pearson's _r_ = `r jh.m1_diffdiff_cor_r`, _p_ = `r jh.m1_diffdiff_cor_p`, Effect 2 (pre-verb-offset): Pearson's _r_ = `r jh.m2_diffdiff_cor_r`, _p_ = `r jh.m2_diffdiff_cor_p`, Effect 3 (first fixation): Pearson's _r_ = `r jh.m3_diff_cor_r`, _p_ = `r jh.m3_diff_cor_p`. However, when the two interaction effects are calculated as the target advantage in the restricting condition only (i.e. rather than a difference of differences), we see a significant correlation between target advantage and calibration quality in the wider pre-noun window (Pearson's _r_ = `r jh.m1_diff_cor_r`, _p_ = `r jh.m1_diff_cor_p`).