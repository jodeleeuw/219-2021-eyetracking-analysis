---
title: "E4-analysis-ROIs"
output: pdf_document
---

```{r , include=FALSE}
library(papaja)
library(jsonlite)
library(tidyverse)
library(lmerTest)
library(afex)
library(forcats)
library(broom.mixed)
```

```{r in-box-function-ROIs}
#Define a function for determining if `x,y` value falls in box.
in.box <- function(x, y, left, right, top, bottom, padding){
  is.in.the.box <- x >= left - padding & x <= right + padding & y >= top - padding & y <= bottom + padding
  return(is.in.the.box)
}
```

```{r E4-load-data-ROIs}
data.files <- list.files('data/run-2', full.names = TRUE)
data.tables <- lapply(data.files, function(file){
  data.table <- fromJSON(file)
  return(data.table)
})
all.data <- bind_rows(data.tables)
```


```{r }
task.data <- all.data %>%
  dplyr::filter(compatibility != 'NA', compatibility != 'filler') %>%
  dplyr::select(subject, trial_index, rt, images, webgazer_data, mouse_events, compatibility, audio, target_instrument, target_animal, webgazer_targets)

screen.data <- all.data %>%
  dplyr::select(subject, screen_height, screen_width) %>% 
  filter(!is.na(screen_height)) %>% 
  unique() %>% 
  group_by(subject) %>% 
  mutate(n()) %>% # one subject has 2 rows because width is different by less than a pixel. I'll just keep one
  sample_n(size=1)


# Add a column that uniquely identifies the combination of images and audio shown on the screen
trialID.data <- task.data %>%
  group_by(audio, images) %>%
  slice(1) %>%
  select(audio, images) %>%
  ungroup() %>%
  mutate(trialID = 1:n())

task.data <- task.data %>%
  left_join(trialID.data)

eyetracking.data <- task.data %>%
  tidyr::unpack(webgazer_targets) %>%
  tidyr::unpack(c(`#jspsych-free-sort-draggable-0`, `#jspsych-free-sort-draggable-1`, `#jspsych-free-sort-draggable-2`, `#jspsych-free-sort-draggable-3`), names_sep=".") %>%
  unnest(webgazer_data)

mousetracking.data <- task.data %>%
  unnest(mouse_events)
```

Coordinates need to be fixed!

```{r E4-categorizing-gaze-location-ROIs}
#First figure out which object they are looking at.
#Calculate gaze in ROIs.

eyetracking.data.with.roi <- eyetracking.data %>%
  left_join(screen.data, by = 'subject') %>% 
  #filter(!subject %in% bad.eyetracking.data.subjects) %>% 
  filter(x >= 0, y >= 0) %>% # negative values indicate something's incorrect (drops about 12,000 rows!!)
  mutate(in.roi.0 = in.box(x,y, left = 0, right = screen_width/2, top = 0, bottom = screen_height/2, padding = -25)) %>%
  mutate(in.roi.1 = in.box(x,y, left = screen_width/2, right = screen_width, top = 0, bottom = screen_height/2, padding = -25)) %>%
  mutate(in.roi.2 = in.box(x,y, left = 0, right = screen_width/2, top = screen_height/2 , bottom = screen_height, padding = -25)) %>%
  mutate(in.roi.3 = in.box(x,y, left = screen_width/2, right = screen_width, top = screen_height/2 , bottom = screen_height, padding = -25)) %>% 
  mutate(in.roi.instrument = case_when(
    target_instrument == '#jspsych-free-sort-draggable-0' | target_instrument == '#jspsych-freesort-draggable-0'  ~ in.roi.0,
    target_instrument == '#jspsych-free-sort-draggable-1' | target_instrument == '#jspsych-freesort-draggable-1' ~ in.roi.1,
    target_instrument == '#jspsych-free-sort-draggable-2' | target_instrument == '#jspsych-freesort-draggable-2' ~ in.roi.2,
    target_instrument == '#jspsych-free-sort-draggable-3' | target_instrument == '#jspsych-freesort-draggable-3' ~ in.roi.3
  )) %>%
  mutate(in.roi.animal = case_when(
    target_animal == '#jspsych-free-sort-draggable-0' | target_animal == '#jspsych-freesort-draggable-0' ~ in.roi.0,
    target_animal == '#jspsych-free-sort-draggable-1' | target_animal == '#jspsych-freesort-draggable-1' ~ in.roi.1,
    target_animal == '#jspsych-free-sort-draggable-2' | target_animal == '#jspsych-freesort-draggable-2' ~ in.roi.2,
    target_animal == '#jspsych-free-sort-draggable-3' | target_animal == '#jspsych-freesort-draggable-3' ~ in.roi.3
  )) %>% 
  #sample_n(100) %>% 
  rowwise() %>% 
  mutate(roi_vec = list(c(in.roi.0, in.roi.1, in.roi.2, in.roi.3)),
         which_roi = ifelse(sum(roi_vec)==1, which(unlist(roi_vec) == TRUE), 99 ) ) 
```

```{r}

ggplot(eyetracking.data.with.roi %>% filter(subject == '5564e577fdf99b6c901a75a6'))+
  geom_point(aes(x = x, y = y, color = as.factor(which_roi)))+
  scale_y_reverse()
```
```{r}

ggplot(eyetracking.data.with.roi %>% filter(subject == '5564e577fdf99b6c901a75a6'))+
  geom_point(aes(x = x, y = y, color = as.factor(which_roi)), alpha = 0.5)+
  facet_grid(compatibility~target_animal)+
  scale_y_reverse()
```

```{r E4-load-audio-timing-data-ROIs, message=FALSE}
# load audio data
audio.info <- read_csv('info/audio_timing.csv')
```


```{r E4-average-onsets-of-critical-words-ROIs}
#Calculate average animal onset
animal.onset <- audio.info %>% pull(onset_noun) %>% mean()
instrument.onset <- audio.info %>% pull(onset_instrument) %>% mean()
```

```{r E4-timelocking-gaze-data-ROIs}
# Merge in audio timing information
eyetracking.data.with.roi <- eyetracking.data.with.roi %>%
  mutate(sound = str_split(audio, pattern="/", simplify = T)[,4])

eyetracking.data.with.roi <- eyetracking.data.with.roi %>%
  left_join(audio.info, by="sound")

# Add time window information
eyetracking.data.with.time.windows <- eyetracking.data.with.roi %>%
  ungroup() %>% 
  #dplyr::slice_sample(n=100) %>% 
  mutate(time.window = case_when(
    t < onset_verb + 200 ~ "pre-verb-onset",
    t <= onset_noun + 200 ~ "post-verb-onset-pre-animal-onset",
    t <= onset_instrument + 200 ~ "post-animal-onset-pre-instrument-onset",
    t <= onset_instrument + 1500 + 200 ~ "post-instrument-onset",
    TRUE ~ "end"
  ),
  time.from.verb = t - onset_verb)
```

```{r}

ggplot(eyetracking.data.with.time.windows %>% filter( time.window == "post-animal-onset-pre-instrument-onset"))+
  geom_vline(aes(xintercept=screen_width/2))+
  geom_hline(aes(yintercept=screen_height/2))+
  geom_point(aes(x = x, y = y, color = as.factor(in.roi.animal)), alpha=0.5)+
  facet_grid(compatibility~target_animal)+
  scale_y_reverse()
```
```


```{r E4-downsampling-gaze-data-for-plotting-ROIs}
#Add time window
eyetracking.data.with.time.windows <- eyetracking.data.with.time.windows %>%
  mutate(t.window = floor(time.from.verb/50)*50)
```

```{r E4-summarizing-gaze-data-for-plotting-ROIs}
#Summarize data for plotting
eyetracking.figure.2.data <- eyetracking.data.with.time.windows %>%
  filter(between(t.window , -200, 4000)) %>%
  group_by(subject, compatibility, t.window) %>%
  summarize(p.animal = mean(in.roi.animal), p.instrument = mean(in.roi.instrument)) %>%
  pivot_longer(c('p.animal', 'p.instrument'), names_to="object_type", values_to="prop_fixations") %>%
  #mutate(prop_fixations = if_else(is.na(prop_fixations), 0, prop_fixations)) %>%
  group_by(compatibility, t.window, object_type) %>%
  summarize(M=mean(prop_fixations), SE=sd(prop_fixations)/sqrt(n()))
```

```{r E4-gaze-timecourse-fig-ROIs, fig.cap = "Timecourse of eye-gaze to target animal and target instrument by verb bias condition. Vertical lines indicate average onsets of animal and instrument offset by 200ms."}

fig2<-ggplot(eyetracking.figure.2.data %>% 
  mutate(compatibility = factor(compatibility, 
                                levels = c("modifier", "equibiased", "instrument" ), 
                                labels = c("Modifier", "Equi-biased", "Instrument" ))), 
  aes(x=t.window, y=M, ymin=M-SE, ymax=M+SE, color=compatibility, fill=compatibility, linetype=object_type))+
  geom_ribbon(color=NA, alpha=0.3)+
  geom_line(size=1)+
  scale_color_brewer(palette = "Set1")+
  scale_fill_brewer(palette = "Set1")+
  scale_linetype(labels = c("Animal", "Instrument") )+
  theme_classic() +
  geom_vline(xintercept = animal.onset + 200) + 
  geom_vline(xintercept = instrument.onset + 200)+
  labs(y = "Proportion of looks", x = "Time relative to verb onset (ms)")+
  guides(color = guide_legend("Verb bias"),fill = guide_legend("Verb bias"), linetype = guide_legend("Gaze location"))
fig2
```

```{r }
#Summarize fixations on target and instrument
eyetracking.window.summary.by.trial <- eyetracking.data.with.time.windows %>%
  group_by(subject, trialID, sound, compatibility, time.window) %>%
  summarize(prop.fixations.animal = sum(in.roi.animal) / n(),
            prop.fixations.instrument = sum(in.roi.instrument) / n()) %>%
  mutate(compatibility = factor(compatibility))
```



```{r }
# Add orthogonal contrasts to model
contrasts(eyetracking.window.summary.by.trial$compatibility) <- cbind(c(-2/3, 1/3, 1/3), c(0, -1/2, 1/2))
```


```{r }
data.time.window.1 <- eyetracking.window.summary.by.trial %>% filter(time.window == "post-verb-onset-pre-animal-onset")

model.time.window.1 <- lmer(prop.fixations.animal ~ compatibility + (1  | subject) + (1 | trialID), data=data.time.window.1,
                            control = lmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 2e6)))

```

```{r }
data.time.window.2 <- eyetracking.window.summary.by.trial %>% filter(time.window == "post-animal-onset-pre-instrument-onset")

model.time.window.2 <- lmer(prop.fixations.animal ~ compatibility + (1  | subject) + (1 | trialID), data=data.time.window.2)
```

```{r }
data.time.window.3 <- eyetracking.window.summary.by.trial %>% filter(time.window == "post-instrument-onset")

model.time.window.3 <- lmer(prop.fixations.animal ~ compatibility + (1 | subject) + (1 | trialID), data=data.time.window.3)
```
```{r E4-ET-analysis-tables}
E4_ET_model1_tab = broom.mixed::tidy(model.time.window.1) 
E4_ET_model2_tab = broom.mixed::tidy(model.time.window.2) 
E4_ET_model3_tab = broom.mixed::tidy(model.time.window.3) 

E4_ET_model1_c1 = E4_ET_model1_tab %>% filter(term == "compatibility1")
E4_ET_model1_c2 = E4_ET_model1_tab %>% filter(term == "compatibility2")

E4_ET_model2_c1 = E4_ET_model2_tab %>% filter(term == "compatibility1")
E4_ET_model2_c2 = E4_ET_model2_tab %>% filter(term == "compatibility2")

E4_ET_model3_c1 = E4_ET_model3_tab %>% filter(term == "compatibility1")
E4_ET_model3_c2 = E4_ET_model3_tab %>% filter(term == "compatibility2")
```


