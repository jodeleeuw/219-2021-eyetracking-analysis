---
title: "E4-analysis-replication-in-lab"
author: "Rachel Ryskin"
date: ""
output: pdf_document
---

```{r , include=FALSE}
# THIS IS A VERY MESSY RMD IN WHICH I TRIED TO ANALYSE THE IN-LAB REPLICATION DATA: DO NOT USE, I WILL EVENTUALLY REMOVE IT - RR
# subject identifiers did not get saved (and some other important stuff too) so the data cannot be analyzed in a reasonable way
library(papaja)
library(jsonlite)
library(purrr)
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)
library(stringr)
library(lmerTest)
library(afex)
library(forcats)
library(broom.mixed)
library(lubridate)
```

```{r in-box-function}
#Define a function for determining if `x,y` value falls in box.
in.box <- function(x, y, left, right, top, bottom, padding){
  is.in.the.box <- x >= left - padding & x <= right + padding & y >= top - padding & y <= bottom + padding
  return(is.in.the.box)
}
```

```{r E4-in-lab-load-data-first-batch}

#  without dates for user ids
add_order = function(trial_index, lag_trial_index, lag_trial_order, verbose = F){
  
  if( length(lag_trial_index)==0 | length(lag_trial_order)==0 ){
    trial_order = 0
  }else if(trial_index == lag_trial_index){
    trial_order = lag_trial_order
  }else if(trial_index < lag_trial_index){
    trial_order = 0
  }else if(trial_index > lag_trial_index){
    trial_order = lag_trial_order+1
  }
  
  if (verbose){
    cat("\n trial index: ",trial_index,"\n ")
    cat("lag trial index: ",lag_trial_index,"\n ")
    cat("lag trial order: ",lag_trial_order,"\n ")
    cat("new trial order: ", trial_order)
  }
   return(trial_order)
}
#add_order(3,1,1,T)



batch1<-readRDS("data/in-lab-merced/verb_bias_ET_data_10_13_2022.rds") %>% 
  tibble::rownames_to_column("row_order") %>% 
  mutate(row_order = as.numeric(row_order)) %>% 
  separate(internal_node_id, into=c('node0','node1', 'node2', 'node3'), sep = "-",remove = F) %>%
  mutate(across(starts_with("node"),as.numeric)) %>% 
  arrange(user_id, row_order) #%>% 
  #group_by(user_id, node0,node1,node2,node3) %>% 
 # group_by(user_id) %>%
 # mutate(trial_order  = 0)

subj_orders = vector()
subjnum = 1
for (i in 1:length(batch1$row_order)){
  if(str_detect(batch1$stimulus[i], "hypothesis")){
    subjnum = subjnum+1
  }
  subj_orders[i] = subjnum
}
#add_order(3,1,1,T)

batch1z = batch1 
batch1z$subj_order = subj_orders

batch1z = batch1 %>% 
     arrange(user_id, trial_index, row_order) %>% 
    # mutate(prev_trial_end = time_elapsed - rt - 2) %>% 
     group_by(user_id, trial_index) %>% 
     mutate(daily_subjnum = 1:n(),
             subj_order = str_c(daily_subjnum,"__", user_id)) #%>% 

dayxz = batch1z %>% filter(user_id == 'c402ba0c-7a9d-4258-addd-149223724707')  %>% 
  select(-starts_with('webgazer')) %>% 
  ungroup() %>% 
  mutate(time_lag = as.numeric(time_elapsed) - as.numeric(lag(time_elapsed)))

```
# BATCH1Z
```{r}
batch1z.task.data = batch1z %>%
  dplyr::filter(compatibility != 'NA', compatibility != 'filler') %>%
  dplyr::select(subj_order, trial_index, rt, images, webgazer_data, mouse_events, compatibility, audio, target_instrument, target_animal, webgazer_targets)

# Add a column that uniquely identifies the combination of images and audio shown on the screen
batch1z.trialID.data <- batch1z.task.data %>%
  group_by(audio, images) %>%
  slice(1) %>%
  select(audio, images) %>%
  ungroup() %>%
  mutate(trialID = 1:n())

batch1z.task.data <- batch1z.task.data %>%
  left_join(batch1z.trialID.data)

batch1z.eyetracking.data <- batch1z.task.data %>%
  tidyr::unpack(webgazer_targets) %>%
  tidyr::unpack(c(`#jspsych-free-sort-draggable-0`, `#jspsych-free-sort-draggable-1`, `#jspsych-free-sort-draggable-2`, `#jspsych-free-sort-draggable-3`), names_sep=".") %>%
  unnest_wider(webgazer_data) %>% 
  unnest_longer(col = c(x, y, t))

batch1z.mousetracking.data <- batch1z.task.data %>%
  unnest(mouse_events)
```
```{r E4-categorizing-first-mouse-moves}
#First, create a data set with each trial, adding column for which object the mouse moves over first.
# WTF IS HAPPENING with TRIAL ID?? 
batch1z.first.move <- batch1z.mousetracking.data %>%
  select(subj_order, trialID, object, type, compatibility, target_instrument, target_animal, images, audio) %>%
  group_by(subj_order, trialID) %>%
  filter(type=="enter") %>%
  slice(1) %>%
  rowwise() %>%
  mutate(which_loc = which(images == object)-1) %>%
  ungroup() %>%
  mutate(instrument_loc = str_sub(target_instrument, start=-1, end=-1)) %>%
  mutate(animal_loc = str_sub(target_animal, start=-1, end=-1)) %>%
  mutate(first.move.type = case_when(
    which_loc == instrument_loc ~ 'instrument',
    which_loc == animal_loc ~ 'animal',
    TRUE ~ 'other')) %>%
  mutate(is.mouse.instrument = if_else(first.move.type == 'instrument', 1, 0)) %>%
  mutate(compatibility = factor(compatibility))
```


```{r E4-summarizing-first-mouse-moves-for-plotting}
# Summarize the data by subject, calculating proportion of trials the the first move was to the animal, instrument, or other.
batch1z.first.move.subject.summary <- batch1z.first.move %>%
  group_by(subj_order, compatibility) %>%
  summarize(prop.animal = mean(first.move.type == 'animal'),
            prop.instrument = mean(first.move.type == 'instrument'),
            prop.other = mean(first.move.type == 'other'),
            n_trials= n()) %>%
  filter(n_trials > 9) %>% ## CAHNGE WHEN THERE'S A BETTER WAY TO CROSSREF
  pivot_longer(c('prop.animal', 'prop.instrument', 'prop.other'), names_to="target_type", values_to="proportion")

# Summarize the condition-level data for a barplot.
batch1z.first.move.summary <- batch1z.first.move.subject.summary %>%
  group_by(compatibility, target_type) %>%
  summarize(M=mean(proportion), SE=sd(proportion)/sqrt(n())) %>% 
  mutate(bias = factor(compatibility, levels = c("instrument", "equibiased", "modifier") ))
```


```{r E4-mouse-moves-fig, fig.cap = "Proportion of first mouse movements by location and verb bias."}

ggplot(batch1z.first.move.summary, aes(x=bias, fill=target_type, y=M, ymin=M-SE,ymax=M+SE))+
  geom_col(position=position_dodge(width=0.9), color = "black")+
  geom_errorbar(position=position_dodge(width=0.9), width=0.2)+
  scale_fill_brewer(palette = "Set1", name = "Location", labels = c("Animal", "Instrument", "Other"))+
  coord_cartesian(ylim = c(0,1))+
  theme_classic()+
  labs(y = "Proportion of first mouse movements", x = NULL)
```



```{r}

for (r in 1:length(batch1$trial_index)){
  #print(r)
  #print(add_order(batch1$trial_index[r], batch1$trial_index[r-1], batch1$trial_order[r-1],verbose=T))
  batch1$trial_order[r] = add_order(batch1$trial_index[r], batch1$trial_index[r-1], batch1$trial_order[r-1],F)
  
}
 
# now connecting trials to each other based on elapsed time and rt

#View(batch1 %>% select(trial_index, internal_node_id,user_id, trial_order))
batch1a = batch1 %>%   
    arrange(user_id, trial_order, time_elapsed) %>% 
    mutate(prev_trial_end = time_elapsed - rt - 2) %>% 
    group_by(user_id, trial_index) %>% 
    mutate(daily_subjnum = 1:n(),
            unique_id = str_c(daily_subjnum,"__", user_id)) #%>% 
    # mutate(prev_trial_id = case_when(
    #   prev_trial_end == time_elapsed
    # ))
    # ungroup() %>% 
    # arrange(unique_id, trial_index) 

link_trials  = function(s, to, df, tol=1, verbose=F){
  
  prev = df %>% filter(user_id == s & trial_order == to-1)
  curr = df %>% filter(user_id == s & trial_order == to)
  linked = vector("character", length(curr$prev_trial_end))
  
  for (q in 1:length(curr$prev_trial_end)){
    if(verbose){cat("q", q, ": ", curr$prev_trial_end[q],"\n")}
    for (p in 1:length(prev$time_elapsed)){
      if(verbose){cat("p", p, ": ", prev$time_elapsed[p],"\n")}
      if (!is.na(prev$time_elapsed[p]) & !is.na(curr$prev_trial_end[q])){
        if (curr$prev_trial_end[q]==prev$time_elapsed[p]| 
            (curr$prev_trial_end[q]<=prev$time_elapsed[p]+tol &
            curr$prev_trial_end[q]>=prev$time_elapsed[p]-tol)){
          if(verbose){cat("p",p, ": ", prev$unique_id[p],"\n")}
          linked[q] = prev$unique_id[p]
        }
      }
    }
  }
  return(linked)
}

#link_trials('6a935167-a0f5-4399-b6dd-8928fe8bae35',2,batch1a, tol=1)
link_trials('0f3402df-0da9-4a51-ac92-6a8c8f8e8c48',12,batch1a, tol=1)

linked_ids = vector("character")
for(user in unique(batch1a$user_id)){
  cat("\n\nUSER: ",user,"\n\n")
  for (trial in unique(batch1a[batch1a$user_id == user,]$trial_order)){
    cat("trial: ", trial,"\n")
    if(trial == 0){
      linked_ids = c(linked_ids, batch1a[batch1a$user_id == user & batch1a$trial_order == 0,]$unique_id)
    }else{
      linked_ids = c(linked_ids, link_trials(s=user, to = trial, df=batch1a, tol=1))
    }
    
  }
} 

batch1b = batch1a
batch1b$linked_unique_ids  = linked_ids

dayx = batch1b %>% filter(user_id == 'c402ba0c-7a9d-4258-addd-149223724707')  %>% select(-starts_with('webgazer'))
```

```{r}

#OKAY NOW NEED TO FILL IN LINKED IDS FOR THE CRITICAL TRIALS USING RTS FROM MOUSE CLICKS???

batch1.mouse = batch1b %>% 
  unnest(mouse_events,keep_empty = T) %>% 
  # mutate(prev_elapsed = time_elapsed - rt, 
  #          rt_eye = audio_start_time + t) %>% 
  group_by(user_id,trial_index,time_elapsed) %>% 
  mutate(prev_elapsed = time_elapsed-max(t))
  
dayx.mouse = batch1.mouse %>% filter(user_id == 'c402ba0c-7a9d-4258-addd-149223724707') %>% 
  select(-starts_with('webgazer')) %>% 
  select(rt, trial_index,trial_type, internal_node_id, time_elapsed, audio_start_time, type, object, t, prev_elapsed)
# 
# write_csv(dayx, "data/in-lab-merced/test.csv")

dayx.summ = dayx %>% group_by(internal_node_id) %>% summarize(n())

batch1.summary = batch1 %>% 
  dplyr::filter(compatibility != 'NA', compatibility != 'filler') %>%
  group_by(user_id, daily_subjnum, unique_id) %>% 
  summarize(n=n()) %>% 
  group_by(user_id) %>% 
  mutate(daily_total = max(daily_subjnum)) %>% 
  arrange( user_id, daily_subjnum) 


```

```{r}
batch1.gaze = batch1b %>% 
  tidyr::unpack(webgazer_targets) %>%
  tidyr::unpack(c(`#jspsych-free-sort-draggable-0`, `#jspsych-free-sort-draggable-1`, `#jspsych-free-sort-draggable-2`, `#jspsych-free-sort-draggable-3`), names_sep=".") %>%
  unnest_wider(webgazer_data) %>% 
  unnest_longer(col = c(x, y, t)) 

dayx.gaze = batch1.gaze %>% filter(user_id == 'c402ba0c-7a9d-4258-addd-149223724707') %>% 
  ungroup() %>% 
  mutate(trial_lag = as.numeric(time_elapsed) - as.numeric(lag(time_elapsed))
        # ,last_gaze = t[[1]][length(t[[1]])]
         ) 


for (i in 1:length(dayx.gaze$t)){
  g = dayx.gaze$t[i][[1]][length(dayx.gaze$t[i][[1]])]
  if(is.null(g)){
    last_gaze[i]<-NA
  }else{
    last_gaze[i]<-g
  }
}

dayx.gaze = cbind(dayx.gaze,last_gaze) 
dayx.gaze = dayx.gaze %>%  
  mutate(extra = trial_lag - last_gaze) %>% 
  select(rt, trial_index, trial_type,time_elapsed,trial_lag,audio_start_time,t,last_gaze, extra)
```

#batch2

```{r E4-in-lab-load-data-second-batch}
# second batch

user.data.files <- list.files('data/in-lab-merced', pattern = "pushkin_users_*",full.names = TRUE)

all.user.data <- map_dfr(user.data.files, readRDS) %>% unique()

data.files <- list.files('data/in-lab-merced', pattern = "lex_*",full.names = TRUE)
# data.tables <- lapply(data.files, function(file){
#   data.table <- fromJSON(file)
#   return(data.table)
# })

#data.tables = readRDS("data/in-lab-merced/lex_stimulusResponses_2023-05-01.rds")
all.data <- map_dfr(data.files, readRDS) %>% 
  bind_rows(readRDS("data/in-lab-merced/verb_bias_ET_data_10_13_2022.rds")) %>% 
  arrange(user_id, trial_index, time_elapsed) %>% 
  group_by(user_id, trial_index) %>% 
  mutate(daily_subjnum = 1:n(),
         unique_id = str_c(daily_subjnum,"__", user_id)) %>% 
  ungroup() %>% 
  arrange(unique_id, trial_index) %>% 
  left_join(all.user.data, by = "user_id") %>% 
 # separate(created_at, into = c("date", "time"), sep = " ") %>% 
  mutate(date_time = parse_date_time(created_at, "ymd HMS")) %>% 
  mutate( date = date(date_time))


```

*TO DO:* cross ref with angela's list of participant times/dates
-still trying to figure out how the daes/times line up...

```{r}

participant_log = read_csv('data/in-lab-merced/participant_log.csv') %>% 
  separate(time, into = c("start_time", "end_time"), sep = "-") %>% 
  mutate(end_date_time = paste(date, end_time, sep = " "),
        PSTtime = strptime(end_date_time, format = "%m/%d/%y %H:%M", tz = "America/Los_Angeles"),
       # UTCtime = strptime(start_date_time, format = "%m/%d/%y %H:%M", tz = "America/New_York")
       other_tz_time = with_tz(PSTtime, "US/Alaska")
        ) %>% 
         #PSTtime = strptime(start_time, format = "%H:%M", tz = ""),
        # UTCtime = strptime(start_time, format = "%H:%M", tz = "GMT")) %>% 
  filter(action == "credit granted") %>% 
  group_by(date) %>% 
  arrange(PSTtime) %>% 
  mutate(order = 1,
         order = cumsum(order)) 

write_csv(participant_log, "data/in-lab-merced/participant_log2.csv")
  
all.user.data.summary = all.data %>% 
  dplyr::filter(compatibility != 'NA', compatibility != 'filler') %>%
  group_by(date_time, user_id, daily_subjnum, unique_id) %>% 
  summarize(n=n()) %>% 
  arrange(date_time,daily_subjnum) 

write_csv(all.user.data.summary, "data/in-lab-merced/pushkin_user_data_summary.csv")
#hist(all.user.data.summary$n)

#all.user.data.summary %>% ungroup() %>% summarize(n_distinct(date_time), n_distinct(date))
```


### Replication

```{r E4-filter-data-for-replication-analysis}
task.data <- all.data %>%
  dplyr::filter(compatibility != 'NA', compatibility != 'filler') %>%
  dplyr::select(unique_id, trial_index, rt, images, webgazer_data, mouse_events, compatibility, audio, target_instrument, target_animal, webgazer_targets)

# Add a column that uniquely identifies the combination of images and audio shown on the screen
trialID.data <- task.data %>%
  group_by(audio, images) %>%
  slice(1) %>%
  select(audio, images) %>%
  ungroup() %>%
  mutate(trialID = 1:n())

task.data <- task.data %>%
  left_join(trialID.data)

eyetracking.data <- task.data %>%
  tidyr::unpack(webgazer_targets) %>%
  tidyr::unpack(c(`#jspsych-free-sort-draggable-0`, `#jspsych-free-sort-draggable-1`, `#jspsych-free-sort-draggable-2`, `#jspsych-free-sort-draggable-3`), names_sep=".") %>%
  unnest_wider(webgazer_data) %>% 
  unnest_longer(col = c(x, y, t))
```




```{r}

mousetracking.data <- task.data %>%
  unnest(mouse_events)
```

The location of initial mouse movements was used to assess whether the final interpretation of ambiguous sentences was biased by the verb. Figure \@ref(fig:E4-mouse-moves-fig) suggests that listeners were more likely to move their mouse first over the target instrument when the verb was equi-biased than when the verb was modifier-biased and even more so when the verb was instrument-biased. The opposite graded pattern can be observed for mouse movements over the target animal. 


```{r E4-categorizing-first-mouse-moves}
#First, create a data set with each trial, adding column for which object the mouse moves over first.
# WTF IS HAPPENING with TRIAL ID?? 
first.move <- mousetracking.data %>%
  select(unique_id, trialID, object, type, compatibility, target_instrument, target_animal, images, audio) %>%
  group_by(unique_id, trialID) %>%
  filter(type=="enter") %>%
  slice(1) %>%
  rowwise() %>%
  mutate(which_loc = which(images == object)-1) %>%
  ungroup() %>%
  mutate(instrument_loc = str_sub(target_instrument, start=-1, end=-1)) %>%
  mutate(animal_loc = str_sub(target_animal, start=-1, end=-1)) %>%
  mutate(first.move.type = case_when(
    which_loc == instrument_loc ~ 'instrument',
    which_loc == animal_loc ~ 'animal',
    TRUE ~ 'other')) %>%
  mutate(is.mouse.instrument = if_else(first.move.type == 'instrument', 1, 0)) %>%
  mutate(compatibility = factor(compatibility))
```


```{r E4-summarizing-first-mouse-moves-for-plotting}
# Summarize the data by subject, calculating proportion of trials the the first move was to the animal, instrument, or other.
first.move.subject.summary <- first.move %>%
  group_by(unique_id, compatibility) %>%
  summarize(prop.animal = mean(first.move.type == 'animal'),
            prop.instrument = mean(first.move.type == 'instrument'),
            prop.other = mean(first.move.type == 'other'),
            n_trials= n()) %>%
  filter(n_trials > 9) %>% ## CAHNGE WHEN THERE'S A BETTER WAY TO CROSSREF
  pivot_longer(c('prop.animal', 'prop.instrument', 'prop.other'), names_to="target_type", values_to="proportion")

# Summarize the condition-level data for a barplot.
first.move.summary <- first.move.subject.summary %>%
  group_by(compatibility, target_type) %>%
  summarize(M=mean(proportion), SE=sd(proportion)/sqrt(n())) %>% 
  mutate(bias = factor(compatibility, levels = c("instrument", "equibiased", "modifier") ))
```


```{r E4-mouse-moves-fig, fig.cap = "Proportion of first mouse movements by location and verb bias."}

ggplot(first.move.summary, aes(x=bias, fill=target_type, y=M, ymin=M-SE,ymax=M+SE))+
  geom_col(position=position_dodge(width=0.9), color = "black")+
  geom_errorbar(position=position_dodge(width=0.9), width=0.2)+
  scale_fill_brewer(palette = "Set1", name = "Location", labels = c("Animal", "Instrument", "Other"))+
  coord_cartesian(ylim = c(0,1))+
  theme_classic()+
  labs(y = "Proportion of first mouse movements", x = NULL)
```

```{r, eval=FALSE}
#write_csv(first.move, "output/E4_mouse_data.csv")
```

```{r E4-mouse-moves-analysis-lab}
contrasts(first.move$compatibility) <- cbind(c(1/3, -2/3, 1/3), c(-1/2, 0, 1/2))

E4_mouse_moves_model <- glmer(is.mouse.instrument ~ compatibility + (1 + compatibility | unique_id) + (1 | trialID), 
                              data=first.move, family="binomial",
                              glmerControl(optimizer = "bobyqa"))

```

```{r E4-mouse-moves-analysis-table}
E4_mouse_moves_model_tab = broom.mixed::tidy(E4_mouse_moves_model) 
E4_mouse_moves_model_c1 = E4_mouse_moves_model_tab %>% filter(term == "compatibility1")
E4_mouse_moves_model_c2 = E4_mouse_moves_model_tab %>% filter(term == "compatibility2")
```

A mixed-effects logistic regression model was used to predict whether the first movement was on the target instrument with the verb bias condition as an orthogonally contrast-coded (instrument vs. equi & modifier: inst = -2/3, equi = 1/3, mod = 1/3; equi vs. modifier: inst = 0, equi = -1/2, mod = 1/2 ) fixed effect. Participants and items were entered as varying intercepts with by-participant varying slopes for verb bias condition^[`lme4` syntax: `glmer(is.mouse.over.instrument ~ verb_bias + (1 + verb_bias | participant) + (1 | item), family="binomial", data=d)`]. Participants were more likely to first move their mouse over target instruments in the instrument-biased condition relative to the equi-biased and modifier-biased condition (_b_ =  `r E4_mouse_moves_model_c1 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_mouse_moves_model_c1 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.01). Further, participants were more likely to first move their mouse over target instruments in the equi-biased condition relative to the modifier-biased condition (_b_ =  `r E4_mouse_moves_model_c2 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_mouse_moves_model_c2 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.01)


Gaze fixations were time-locked to the auditory stimulus on a trial by trial basis and categorized as being directed towards one of the four items in the display if the x, y coordinates fell within a rectangle containing the image. Figure \@ref(fig:E4-gaze-timecourse-fig) suggests that the participants made more fixations to the target animal when the verb was modifier-biased compared to when the the verb was equi-biased and they looked at the target animal least when the verb was instrument-biased. The pattern was reversed for looks to the target instrument.


```{r E4-categorizing-gaze-location}
#First figure out which object they are looking at.
#Calculate gaze in ROIs.
eyetracking.data.with.roi <- eyetracking.data %>%
  #filter(!subject %in% bad.eyetracking.data.subjects) %>% 
  mutate(in.roi.0 = in.box(x,y,`#jspsych-free-sort-draggable-0.left`, `#jspsych-free-sort-draggable-0.right`, `#jspsych-free-sort-draggable-0.top`, `#jspsych-free-sort-draggable-0.bottom`, 100)) %>%
  mutate(in.roi.1 = in.box(x,y,`#jspsych-free-sort-draggable-1.left`, `#jspsych-free-sort-draggable-1.right`, `#jspsych-free-sort-draggable-1.top`, `#jspsych-free-sort-draggable-1.bottom`, 100)) %>%
  mutate(in.roi.2 = in.box(x,y,`#jspsych-free-sort-draggable-2.left`, `#jspsych-free-sort-draggable-2.right`, `#jspsych-free-sort-draggable-2.top`, `#jspsych-free-sort-draggable-2.bottom`, 100)) %>%
  mutate(in.roi.3 = in.box(x,y,`#jspsych-free-sort-draggable-3.left`, `#jspsych-free-sort-draggable-3.right`, `#jspsych-free-sort-draggable-3.top`, `#jspsych-free-sort-draggable-3.bottom`, 100)) %>%
  mutate(in.roi.instrument = case_when(
    target_instrument == '#jspsych-free-sort-draggable-0' | target_instrument == '#jspsych-freesort-draggable-0'  ~ in.roi.0,
    target_instrument == '#jspsych-free-sort-draggable-1' | target_instrument == '#jspsych-freesort-draggable-1' ~ in.roi.1,
    target_instrument == '#jspsych-free-sort-draggable-2' | target_instrument == '#jspsych-freesort-draggable-2' ~ in.roi.2,
    target_instrument == '#jspsych-free-sort-draggable-3' | target_instrument == '#jspsych-freesort-draggable-3' ~ in.roi.3
  )) %>%
  mutate(in.roi.animal = case_when(
    target_animal == '#jspsych-free-sort-draggable-0' | target_animal == '#jspsych-freesort-draggable-0' ~ in.roi.0,
    target_animal == '#jspsych-free-sort-draggable-1' | target_animal == '#jspsych-freesort-draggable-1' ~ in.roi.1,
    target_animal == '#jspsych-free-sort-draggable-2' | target_animal == '#jspsych-freesort-draggable-2' ~ in.roi.2,
    target_animal == '#jspsych-free-sort-draggable-3' | target_animal == '#jspsych-freesort-draggable-3' ~ in.roi.3
  )) 
```


```{r E4-load-audio-timing-data, message=FALSE}
# load audio data
audio.info <- read_csv('info/audio_timing.csv')
```


```{r E4-average-onsets-of-critical-words}
#Calculate average animal onset
animal.onset <- audio.info %>% pull(onset_noun) %>% mean()
instrument.onset <- audio.info %>% pull(onset_instrument) %>% mean()
```

```{r E4-timelocking-gaze-data}
# Merge in audio timing information
eyetracking.data.with.roi2 <- eyetracking.data.with.roi %>%
  mutate(sound1 = str_split(audio, pattern="/", simplify = T)[,4]) %>% 
  mutate(sound2 = str_split(sound1, pattern=fixed("."), simplify = T)[1]) %>% 
  mutate(sound = paste0(sound2, ".mp3"))

eyetracking.data.with.roi3 <- eyetracking.data.with.roi2 %>%
  left_join(audio.info, by="sound")

# Add time window information
eyetracking.data.with.time.windows <- eyetracking.data.with.roi3 %>%
  mutate(time.window = case_when(
    t < onset_verb + 200 ~ "pre-verb-onset",
    t <= onset_noun + 200 ~ "post-verb-onset-pre-animal-onset",
    t <= onset_instrument + 200 ~ "post-animal-onset-pre-instrument-onset",
    t <= onset_instrument + 1500 + 200 ~ "post-instrument-onset",
    TRUE ~ "end"
  ),
  time.from.verb = t - onset_verb)
```


```{r E4-downsampling-gaze-data-for-plotting}
#Add time window
eyetracking.data.with.time.windows <- eyetracking.data.with.time.windows %>%
  mutate(t.window = floor(time.from.verb/50)*50)
```

```{r E4-summarizing-gaze-data-for-plotting, eval = FALSE, warning = FALSE}
#Summarize data for plotting
eyetracking.data.with.time.windows %>%
  filter(between(t.window , -200, 4000)) %>%
  filter(unique_id %in% unique(first.move.subject.summary$unique_id)) %>% #CHANGE WHEN CROSSREF
  group_by(unique_id, compatibility, t.window) %>%
  summarize(p.animal = mean(in.roi.animal), p.instrument = mean(in.roi.instrument)) %>%
  pivot_longer(c('p.animal', 'p.instrument'), names_to="object_type", values_to="prop_fixations") %>%
  ggplot()+
  stat_summary(aes(x = t.window, y=prop_fixations,  color = object_type), fun="mean")+
  facet_wrap(~unique_id)
  #mutate(prop_fixations = if_else(is.na(prop_fixations), 0, prop_fixations)) %>%
 # group_by(compatibility, t.window, object_type) %>%
 # summarize(M=mean(prop_fixations), SE=sd(prop_fixations)/sqrt(n()))
```

```{r E4-summarizing-gaze-data-for-plotting}
#Summarize data for plotting
eyetracking.figure.2.data <- eyetracking.data.with.time.windows %>%
  filter(between(t.window , -200, 4000)) %>%
  filter(unique_id %in% unique(first.move.subject.summary$unique_id)) %>% #CHANGE WHEN CROSSREF
  group_by(unique_id, compatibility, t.window) %>%
  summarize(p.animal = mean(in.roi.animal), p.instrument = mean(in.roi.instrument)) %>%
  pivot_longer(c('p.animal', 'p.instrument'), names_to="object_type", values_to="prop_fixations") %>%
  #mutate(prop_fixations = if_else(is.na(prop_fixations), 0, prop_fixations)) %>%
  group_by(compatibility, t.window, object_type) %>%
  summarize(M=mean(prop_fixations), SE=sd(prop_fixations)/sqrt(n()))
```

```{r E4-gaze-timecourse-fig, fig.cap = "Timecourse of eye-gaze to target animal and target instrument by verb bias condition. Vertical lines indicate average onsets of animal and instrument offset by 200ms."}

fig2<-ggplot(eyetracking.figure.2.data %>% 
  mutate(compatibility = factor(compatibility, 
                                levels = c("modifier", "equibiased", "instrument" ), 
                                labels = c("Modifier", "Equi-biased", "Instrument" ))), 
  aes(x=t.window, y=M, ymin=M-SE, ymax=M+SE, color=compatibility, fill=compatibility, linetype=object_type))+
  geom_ribbon(color=NA, alpha=0.3)+
  geom_line(size=1)+
  scale_color_manual(values = c( "#377eb8", "#e41a1c","#4daf4a"))+
  scale_fill_manual(values = c("#377eb8", "#e41a1c", "#4daf4a"))+
  scale_linetype(labels = c("Animal", "Instrument") )+
  theme_classic() +
  geom_vline(xintercept = animal.onset + 200) + 
  geom_vline(xintercept = instrument.onset + 200)+
  labs(y = "Proportion of looks", x = "Time relative to verb onset (ms)")+
  guides(color = guide_legend("Verb bias"),fill = guide_legend("Verb bias"), linetype = guide_legend("Gaze location"))
fig2
```
```{r, eval=FALSE}
saveRDS(fig2, "output/ETfig-in-lab.rds")
ggsave("output/ETfig-in-lab.png", plot = fig2, height = 4, width = 6)
```

```{r E4-compute-proportion-gaze-data-by-trial}
#Summarize fixations on target and instrument
eyetracking.window.summary.by.trial <- eyetracking.data.with.time.windows %>%
  group_by(unique_id, trialID, sound, compatibility, time.window) %>%
  summarize(prop.fixations.animal = sum(in.roi.animal) / n(),
            prop.fixations.instrument = sum(in.roi.instrument) / n()) %>%
  mutate(compatibility = factor(compatibility))
```

```{r, eval=FALSE}
write_csv(eyetracking.window.summary.by.trial, "output/E4_eye-tracking_data.csv")
```

```{r E4-set-contrasts-for-ET-analysis}
# Add orthogonal contrasts to model
contrasts(eyetracking.window.summary.by.trial$compatibility) <- cbind(c(-2/3, 1/3, 1/3), c(0, -1/2, 1/2))
```


```{r E4-ET-analysis-verb-to-animal-window}
data.time.window.1 <- eyetracking.window.summary.by.trial %>% filter(time.window == "post-verb-onset-pre-animal-onset")

model.time.window.1 <- lmer(prop.fixations.animal ~ compatibility + (1  | unique_id) + (1 | trialID), data=data.time.window.1,
                            control = lmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 2e6)))

```
```{r E4-ET-analysis-animal-to-inst-window}
data.time.window.2 <- eyetracking.window.summary.by.trial %>% filter(time.window == "post-animal-onset-pre-instrument-onset")

model.time.window.2 <- lmer(prop.fixations.animal ~ compatibility + (1  | unique_id) + (1 | trialID), data=data.time.window.2)
```

```{r E4-ET-analysis-post-inst-window}
data.time.window.3 <- eyetracking.window.summary.by.trial %>% filter(time.window == "post-instrument-onset")

model.time.window.3 <- lmer(prop.fixations.animal ~ compatibility + (1 | unique_id) + (1 | trialID), data=data.time.window.3)
```
```{r E4-ET-analysis-tables}
E4_ET_model1_tab = broom.mixed::tidy(model.time.window.1) 
E4_ET_model2_tab = broom.mixed::tidy(model.time.window.2) 
E4_ET_model3_tab = broom.mixed::tidy(model.time.window.3) 

E4_ET_model1_c1 = E4_ET_model1_tab %>% filter(term == "compatibility1")
E4_ET_model1_c2 = E4_ET_model1_tab %>% filter(term == "compatibility2")

E4_ET_model2_c1 = E4_ET_model2_tab %>% filter(term == "compatibility1")
E4_ET_model2_c2 = E4_ET_model2_tab %>% filter(term == "compatibility2")

E4_ET_model3_c1 = E4_ET_model3_tab %>% filter(term == "compatibility1")
E4_ET_model3_c2 = E4_ET_model3_tab %>% filter(term == "compatibility2")
```


In order to assess how verb bias impacted sentence disambiguation as the sentence unfolded, the proportion of fixations was computed in three time windows: the verb-to-animal window (from verb onset + 200 ms to animal onset + 200 ms), the animal-to-instrument window (from animal onset + 200 ms to instrument onset + 200 ms), and the post-instrument window (from instrument onset + 200 ms to instrument onset + 1500ms + 200 ms). Mixed-effects linear regression models were used to predict the proportions of fixations to the target animal within each time window with the verb bias condition as an orthogonally contrast-coded (instrument vs. equi & modifier: inst = -2/3, equi = 1/3, mod = 1/3; equi vs. modifier: inst = 0, equi = -1/2, mod = 1/2 ) fixed effect. Participants and items were entered as varying intercepts^[`lme4` syntax: `lmer(prop.fix.target.animal ~ verb_bias + (1 + verb_bias | participant) + (1 | item), data=d)`. A model with by-participant varying slopes for verb bias condition was first attempted but did not converge.]. In the _verb-to-noun_ window, participants did not look more at the target animal in any of the verb bias conditions (Instrument vs. Equi and Modifier: _b_ =  `r E4_ET_model1_c1 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_ET_model1_c1 %>% pull(std.error) %>% round(digits = 2)`, _p_ =  `r E4_ET_model1_c1 %>% pull(p.value) %>% round(digits = 2)`; Equi vs. Modifier: _b_ =  `r E4_ET_model1_c2 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_ET_model1_c2 %>% pull(std.error) %>% round(digits = 2)`, _p_ =  `r E4_ET_model1_c2 %>% pull(p.value) %>% round(digits = 2)` ). In the _noun-to-instrument_ window, participants looked more at the target animal in the modifier-biased condition and equi-biased conditions relative to the instrument-biased condition ( _b_ =  `r E4_ET_model2_c1 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_ET_model2_c1 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.01) and in the  modifier biased relative to the equi-biased condition ( _b_ =  `r E4_ET_model2_c2 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_ET_model2_c2 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.05). In the _post-instrument_ window, participants looked more at the target animal in the modifier-biased condition and the equi-biased conditions relative to the instrument-biased condition ( _b_ =  `r E4_ET_model3_c1 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_ET_model3_c1 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.01) but not significantly so in the modifier biased condition relative to the equi-biased condition ( _b_ =  `r E4_ET_model3_c2 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r E4_ET_model3_c2 %>% pull(std.error) %>% round(digits = 2)`, _p_ =  `r E4_ET_model3_c2 %>% pull(p.value) %>% round(digits = 2)`). 