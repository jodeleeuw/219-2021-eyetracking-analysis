---
title: "E4-analysis-calibration"
author: "Rachel Ryskin"
output: pdf_document
---

```{r , include=FALSE}
library(papaja)
library(jsonlite)
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)
library(stringr)
library(lmerTest)
library(afex)
library(forcats)
library(broom.mixed)
```


```{r }
data.files <- list.files('data/run-2', full.names = TRUE)
data.tables <- lapply(data.files, function(file){
  data.table <- fromJSON(file)
  return(data.table)
})
all.data <- bind_rows(data.tables)
```


```{r}

all.data.calib = all.data %>% 
  filter(trial_type == "webgazer-validate") %>% 
  dplyr::select(subject, trial_index,  percent_in_roi, average_offset) %>% 
  tidyr::unnest(percent_in_roi)
  
summary.data.calib = all.data.calib %>% 
  group_by(subject, trial_index) %>% 
  summarize(mean_percent_in_roi = mean(percent_in_roi)) %>% 
  group_by(subject) %>% 
  mutate(calib_num = row_number())
```

```{r, eval = F, out.width="50%", fig.align="center"}
ggplot(summary.data.calib)+
  geom_line(aes(x = calib_num, y = mean_percent_in_roi, color=subject))+
  theme_bw()+
  theme(legend.position = "none") 
```

```{r, eval=F, out.width="50%", fig.align="center"}
summary.data.calib.wide = summary.data.calib %>% 
  select(-trial_index) %>% 
  pivot_wider(id_cols = subject, names_from=calib_num, values_from = mean_percent_in_roi )

# correlation between initial and halfway calibration
ggplot(summary.data.calib.wide %>% filter(is.na(`3`)), aes(x = `1`, y = `2`))+
  geom_point()+
  geom_smooth(method = 'lm')+
  theme_bw()+
  theme(legend.position = "none") 

# correlation between 2 successive calibration attempts
ggplot(summary.data.calib.wide %>% filter(!is.na(`3`)), aes(x = `1`, y = `2`))+
  geom_point()+
  geom_smooth(method = 'lm')+
  theme_bw()+
  theme(legend.position = "none") 

# correlation between second attempt calibration and halfway
ggplot(summary.data.calib.wide %>% filter(!is.na(`3`)), aes(x = `2`, y = `3`))+
  geom_point()+
  geom_smooth(method = 'lm')+
  theme_bw()+
  theme(legend.position = "none") 
```

```{r}

calib.by.subj = summary.data.calib %>% 
  group_by(subject) %>% 
  summarize(mean_percent_in_roi = mean(mean_percent_in_roi))

eyetracking.effects.by.subj = read_csv( "output/E4_eye-tracking_data.csv") %>% 
  rename("condition" = compatibility) %>% 
  filter(time.window == "post-instrument-onset", condition != "filler" ) %>% 
  group_by(condition, time.window, subject) %>% 
  summarize(M = mean(prop.fixations.animal)) %>% 
  pivot_wider(names_from = condition, values_from = M) %>% 
  mutate(bias_effect = modifier - instrument) %>% 
  left_join(calib.by.subj, by = "subject")


```

```{r, eval = F, out.width="50%", fig.align="center"}
ggplot(eyetracking.effects.by.subj, aes(x = mean_percent_in_roi, y = modifier))+
  geom_point()+
  geom_smooth(method = "lm")
```

```{r,eval=F}
write_csv(calib.by.subj,"output/calib_by_subj.csv")
```

```{r}

m_calib = broom::tidy(cor.test(eyetracking.effects.by.subj$mean_percent_in_roi, eyetracking.effects.by.subj$bias_effect))
#m_calib 
```

```{r}
m_calib_r = m_calib %>% pull(estimate ) 

```


```{r E4-calib-corr-plot, fig.cap= 'Calibration scores plotted against the verb bias effect (the difference in target animal fixations between modifier and instrument conditions).', out.width="50%", fig.align="center"}
ggplot(eyetracking.effects.by.subj, aes(x = mean_percent_in_roi, y = bias_effect))+
  geom_point()+
  geom_smooth(method = "lm") +
  labs(x="Calibration Score", y="Bias Effect") +
  theme_bw()+
  theme(panel.grid.minor = element_blank())
```
Participants' calibration quality, measured as the mean percentage of fixations that landed within 200 pixels of the calibration point, varied substantially (`r range(calib.by.subj$mean_percent_in_roi) %>% round(digits = 2)`%). 
The quality of a participant's calibration significantly correlated with the participant's effect size ( _Pearson's r_= `r m_calib_r`, _p_ < 0.05).
The difference in target animal fixation proportions between modifier and instrument conditions was higher for participants with better calibration (see Figure \ \@ref(fig:E4-calib-corr-plot)).


```{r}
eyetracking.window.3 = read_csv( "output/E4_eye-tracking_data.csv") %>% 
  filter(time.window == "post-instrument-onset", compatibility != "filler" ) %>% 
  mutate(condition = factor(compatibility, levels = c('instrument', 'equibiased', 'modifier'))) %>% 
  left_join(calib.by.subj, by = "subject")
```

```{r }
# Add orthogonal contrasts to model
contrasts(eyetracking.window.3$condition) <- cbind(c(-2/3, 1/3, 1/3), c(0, -1/2, 1/2))
```


```{r , eval = F}

model.time.window.3 <- lmer(prop.fixations.animal ~ condition + (1 +condition | subject) + (1 | trialID), data=eyetracking.window.3,
                            control = lmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 2e6)))
summary(model.time.window.3)
```

```{r}
eyetracking.window.3.good.calib = eyetracking.window.3 %>% 
  filter(mean_percent_in_roi >= 50)
```

```{r }
# Add orthogonal contrasts to model
contrasts(eyetracking.window.3.good.calib$condition) <- cbind(c(-2/3, 1/3, 1/3), c(0, -1/2, 1/2))
```


```{r }

m.good.calib <- lmer(prop.fixations.animal ~ condition + (1  | subject) + (1 | trialID), data=eyetracking.window.3.good.calib,
                            control = lmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 2e6)))
#summary(m.good.calib)

m_calib_good = broom.mixed::tidy(m.good.calib) 

m_calib_good_c1 = m_calib_good %>% filter(term == "condition1")
m_calib_good_c2 = m_calib_good %>% filter(term == "condition2")

```
#### Re-analysis After Exclusions
Replicating the linear mixed-effects analysis (in the post-instrument onset time window only) on a subset of 35 participants with calibration quality >50% suggests that the effect of verb bias condition was larger in this subset than in the full dataset. Participants' preference to the target animal relative to the target instrument in the modifier-biased condition and the equi-biased conditions was greater than in the instrument-biased condition ( _b_ =  `r m_calib_good_c1 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r m_calib_good_c1 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.001), but the difference betwen the modifier biased condition and the equi-biased condition was not significant ( _b_ =  `r m_calib_good_c2 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r m_calib_good_c2 %>% pull(std.error) %>% round(digits = 2)`, _p_ =  `r m_calib_good_c2 %>% pull(p.value) %>% round(digits = 2)`). 

```{r}
eyetracking.window.3.great.calib = eyetracking.window.3 %>% 
  filter(mean_percent_in_roi >= 75)
```

```{r }
# Add orthogonal contrasts to model
contrasts(eyetracking.window.3.great.calib$condition) <- cbind(c(-2/3, 1/3, 1/3), c(0, -1/2, 1/2))
```


```{r }

m.great.calib <- lmer(prop.fixations.animal ~ condition + (1  | subject) + (1 | trialID), data=eyetracking.window.3.great.calib,
                            control = lmerControl(optimizer = "bobyqa",
                                                  optCtrl = list(maxfun = 2e6)))
#summary(m.great.calib)
m_calib_great = broom.mixed::tidy(m.great.calib) 

m_calib_great_c1 = m_calib_great %>% filter(term == "condition1")
m_calib_great_c2 = m_calib_great %>% filter(term == "condition2")
```

<!--AJ: Cutting this for consistency with the other Expts
Replicating the linear mixed-effects analysis (in the post-instrument onset time window only) on a subset of 19 participants with calibration quality >75% suggests that the effect of verb bias condition was larger in this subset than in the full dataset. 
Participants looked more at the target animal in the modifier-biased condition and the equi-biased conditions relative to the instrument-biased condition ( _b_ =  `r m_calib_great_c1 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r m_calib_great_c1 %>% pull(std.error) %>% round(digits = 2)`, _p_ <  0.001) but not significantly so in the modifier biased condition relative to the equi-biased condition ( _b_ =  `r m_calib_great_c2 %>% pull(estimate) %>% round(digits = 2)`, _SE_ =  `r m_calib_great_c2 %>% pull(std.error) %>% round(digits = 2)`, _p_ =  `r m_calib_great_c2 %>% pull(p.value) %>% round(digits = 2)`). 
-->