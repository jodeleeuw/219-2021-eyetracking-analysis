
@article{altmannIncrementalInterpretationVerbs1999,
  title = {Incremental Interpretation at Verbs: Restricting the Domain of Subsequent Reference},
  shorttitle = {Incremental Interpretation at Verbs},
  author = {Altmann, Gerry T.M and Kamide, Yuki},
  year = {1999},
  month = dec,
  volume = {73},
  pages = {247--264},
  issn = {00100277},
  doi = {10.1016/S0010-0277(99)00059-1},
  abstract = {Participants' eye movements were recorded as they inspected a semi-realistic visual scene showing a boy, a cake, and various distractor objects. Whilst viewing this scene, they heard sentences such as `the boy will move the cake' or `the boy will eat the cake'. The cake was the only edible object portrayed in the scene. In each of two experiments, the onset of saccadic eye movements to the target object (the cake) was signi\textregistered cantly later in the move condition than in the eat condition; saccades to the target were launched after the onset of the spoken word cake in the move condition, but before its onset in the eat condition. The results suggest that information at the verb can be used to restrict the domain within the context to which subsequent reference will be made by the (as yet unencountered) post-verbal grammatical object. The data support a hypothesis in which sentence processing is driven by the predictive relationships between verbs, their syntactic arguments, and the real-world contexts in which they occur. q 1999 Elsevier Science B.V. All rights reserved.},
  file = {/Users/rachelryskin/Zotero/storage/7SLJBBR9/Altmann and Kamide - 1999 - Incremental interpretation at verbs restricting t.pdf;/Users/rachelryskin/Zotero/storage/DHJ8HEAR/Altmann and Kamide - 1999 - Incremental interpretation at verbs restricting t.pdf;/Users/rachelryskin/Zotero/storage/ELSCL249/Altmann and Kamide - 1999 - Incremental interpretation at verbs restricting t.pdf;/Users/rachelryskin/Zotero/storage/JNBPN5VA/Altmann and Kamide - 1999 - Incremental interpretation at verbs restricting t.pdf},
  journal = {Cognition},
  keywords = {eye-tracking,language comprehension,prediction,semantics,syntax},
  language = {en},
  number = {3}
}


@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2021},
  url = {https://www.R-project.org/},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2020},
  note = {R package version 0.1.0.9997},
  url = {https://github.com/crsh/papaja},
}
@Article{johanssonLookHereEye2014,
  title = {Look {{Here}}, {{Eye Movements Play}} a {{Functional Role}} in {{Memory Retrieval}}},
  author = {Roger Johansson and Mikael Johansson},
  date = {2014-01-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {25},
  pages = {236--242},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797613498260},
  abstract = {Research on episodic memory has established that spontaneous eye movements occur to spaces associated with retrieved information even if those spaces are blank at the time of retrieval. Although it has been claimed that such looks to “nothing” can function as facilitatory retrieval cues, there is currently no conclusive evidence for such an effect. In the present study, we addressed this fundamental issue using four direct eye manipulations in the retrieval phase of an episodic memory task: (a) free viewing on a blank screen, (b) maintaining central fixation, (c) looking inside a square congruent with the location of the to-be-recalled objects, and (d) looking inside a square incongruent with the location of the to-be-recalled objects. Our results provide novel evidence of an active and facilitatory role of gaze position during memory retrieval and demonstrate that memory for the spatial relationship between objects is more readily affected than memory for intrinsic object features.},
  file = {/Users/rachelryskin/Zotero/storage/HIYWBNM7/Johansson and Johansson - 2014 - Look Here, Eye Movements Play a Functional Role in.pdf},
  keywords = {eye-tracking,memory},
  langid = {english},
  number = {1},
}
@InCollection{ryskinLanguagePerspectiveMemory2015,
  title = {Language, {{Perspective}}, and {{Memory}}},
  booktitle = {Emerging {{Trends}} in the {{Social}} and {{Behavioral Sciences}}},
  author = {Rachel Ryskin and Si On Yoon and Sarah Brown-Schmidt},
  editor = {Robert A Scott and Stephan M Kosslyn},
  date = {2015-05-15},
  pages = {1--15},
  publisher = {{John Wiley \& Sons, Inc.}},
  location = {{Hoboken, NJ, USA}},
  doi = {10.1002/9781118900772.etrds0200},
  abstract = {The ability to take the perspective of another person is ubiquitous in many everyday cognitive activities. In particular, it allows people to communicate efficiently with conversational partners. Speakers tailor what they say based on the listener’s knowledge and, likewise, listeners use what they know about the speaker to better understand what the speaker means. In this essay, we review foundational research on the role of perspective-taking in the domain of language processing and describe new lines of work that are beginning to explore the memory processes that support the efficient use of perspectives in conversation. We then discuss key avenues for future research, such as investigating whether the type of perspective-taking involved in creating memory reminders draws on the same underlying cognitive processes as in the domain of language processing. Exploring this interface between language, perspective-taking, and memory will require interdisciplinary crosstalk and integration of methodologies across the domains of memory and language research.},
  file = {/Users/rachelryskin/Zotero/storage/8DV89DUV/Ryskin et al. - 2015 - Language, Perspective, and Memory.pdf;/Users/rachelryskin/Zotero/storage/EJPY7PV8/Ryskin et al. - 2015 - Language, Perspective, and Memory.pdf},
  isbn = {978-1-118-90077-2},
  langid = {english},
}
@Article{ryskinVerbBiasesAre2017,
  title = {Verb Biases Are Shaped through Lifelong Learning.},
  author = {Rachel Ryskin and Zhenghan Qi and Melissa C. Duff and Sarah Brown-Schmidt},
  date = {2017-05},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {43},
  pages = {781--794},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000341},
  abstract = {Verbs often participate in more than 1 syntactic structure, but individual verbs can be biased in terms of whether they are used more often with 1 structure or the other. For instance, in a sentence such as “Bop the bunny with the flower,” the phrase “with the flower” is more likely to indicate an instrument with which to “bop,” rather than which “bunny” to bop. Conversely, in a sentence such as “Choose the cow with the flower,” the phrase “with the flower” is more likely to indicate which “cow” to choose. An open question is where these biases come from and whether they continue to be shaped in adulthood in a way that has lasting consequences for real-time processing of language. In Experiment 1 we replicated previous findings that these language-wide biases guide online syntactic processing in a computer-based visual-world paradigm. In Experiment 2, we tested the malleability of these biases by exposing adults to initially unbiased verbs situated in unambiguous contexts that led to either instrument or modifier interpretations. During test, participants interpreted sentences containing either modifier- or instrument-trained verbs in ambiguous contexts. Eyemovement and action data show that participants’ considerations of the candidate interpretations of the ambiguous with-phrases were guided by the newly learned verb biases. These results suggest that co-occurrence information about specific verbs and syntactic structures embedded in language experiences plays a role in forming, and continuously shaping, the verb biases that constitute a part of the broader representation of the language.},
  file = {/Users/rachelryskin/Zotero/storage/5JDZUAKL/Ryskin et al. - 2017 - Verb biases are shaped through lifelong learning..pdf;/Users/rachelryskin/Zotero/storage/7JZV347G/Ryskin et al. - 2017 - Verb biases are shaped through lifelong learning..pdf;/Users/rachelryskin/Zotero/storage/AI3VC78U/Ryskin et al. - 2017 - Verb biases are shaped through lifelong learning..pdf;/Users/rachelryskin/Zotero/storage/BUS4TKHS/Ryskin et al. - 2017 - Verb biases are shaped through lifelong learning..pdf;/Users/rachelryskin/Zotero/storage/IN2Q5NMQ/Ryskin et al. - 2017 - Verb biases are shaped through lifelong learning..pdf;/Users/rachelryskin/Zotero/storage/L5VM5NUE/Ryskin et al. - 2017 - Verb biases are shaped through lifelong learning..pdf},
  langid = {english},
  number = {5},
}

@Article{ryskinVerbBiasesAre2017,
  title = {Verb Biases Are Shaped through Lifelong Learning.},
  author = {Rachel Ryskin and Zhenghan Qi and Melissa C. Duff and Sarah Brown-Schmidt},
  date = {2017-05},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {43},
  pages = {781--794},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000341},
  abstract = {Verbs often participate in more than 1 syntactic structure, but individual verbs can be biased in terms of whether they are used more often with 1 structure or the other. For instance, in a sentence such as “Bop the bunny with the flower,” the phrase “with the flower” is more likely to indicate an instrument with which to “bop,” rather than which “bunny” to bop. Conversely, in a sentence such as “Choose the cow with the flower,” the phrase “with the flower” is more likely to indicate which “cow” to choose. An open question is where these biases come from and whether they continue to be shaped in adulthood in a way that has lasting consequences for real-time processing of language. In Experiment 1 we replicated previous findings that these language-wide biases guide online syntactic processing in a computer-based visual-world paradigm. In Experiment 2, we tested the malleability of these biases by exposing adults to initially unbiased verbs situated in unambiguous contexts that led to either instrument or modifier interpretations. During test, participants interpreted sentences containing either modifier- or instrument-trained verbs in ambiguous contexts. Eyemovement and action data show that participants’ considerations of the candidate interpretations of the ambiguous with-phrases were guided by the newly learned verb biases. These results suggest that co-occurrence information about specific verbs and syntactic structures embedded in language experiences plays a role in forming, and continuously shaping, the verb biases that constitute a part of the broader representation of the language.},
  langid = {english},
  number = {5},
}
@Article{mannsVisualPairedcomparisonTask2000,
  title = {The Visual Paired-Comparison Task as a Measure of Declarative Memory},
  author = {Joseph R. Manns and Craig E. L. Stark and Larry R. Squire},
  date = {2000-10-24},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {97},
  pages = {12375--12379},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.220398097},
  abstract = {Performance on the visual paired-comparison task depends on the integrity of the hippocampal formation in humans, monkeys, and, for an analogous task, in rats. The present study sought additional evidence in healthy volunteers concerning the nature of this task. We found that performance on the visual paired-comparison task was predictive of subsequent recognition memory performance whereas perceptual priming was unrelated to subsequent recognition memory performance. The results are consistent with the data from lesions and suggest that performance on the visual paired-comparison task measures a form of declarative memory.},
  eprint = {11027310},
  eprinttype = {pmid},
  file = {/Users/rachelryskin/Zotero/storage/QWYMGEEM/Manns et al. - 2000 - The visual paired-comparison task as a measure of .pdf;/Users/rachelryskin/Zotero/storage/86GIKB4W/12375.html},
  langid = {english},
  number = {22},
}
@Article{shimojoGazeBiasBoth2003,
  title = {Gaze Bias Both Reflects and Influences Preference},
  author = {Shinsuke Shimojo and Claudiu Simion and Eiko Shimojo and Christian Scheier},
  date = {2003-12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {6},
  pages = {1317--1322},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1150},
  file = {/Users/rachelryskin/Zotero/storage/HCUL243S/Shimojo et al. - 2003 - Gaze bias both reflects and influences preference.pdf},
  langid = {english},
  number = {12},
}
@Article{tanenhaus1995,
  ids = {tanenhausIntegrationVisualLinguistic1995,tanenhausTanenhaus1995Science},
  title = {Integration of Visual and Linguistic Information in Spoken Language Comprehension},
  author = {Michael K Tanenhaus and Michael J Spivey-Knowlton and Kathleen M Eberhard and Julie C Sedivy},
  date = {1995},
  journaltitle = {Science},
  volume = {268},
  pages = {1632--1634},
  publisher = {{American Association for the Advancement of Science}},
  eprint = {7777863},
  eprinttype = {pmid},
  file = {/Users/rachelryskin/Zotero/storage/JW6UJ6PQ/Tanenhaus(1995)Science.pdf;/Users/rachelryskin/Zotero/storage/LN97T6RB/1632.html},
  number = {5217},
}
@Article{spiveyOculomotorMechanismsActivated2001,
  title = {Oculomotor Mechanisms Activated by Imagery and Memory: Eye Movements to Absent Objects},
  shorttitle = {Oculomotor Mechanisms Activated by Imagery and Memory},
  author = {Michael J. Spivey and Joy J. Geng},
  date = {2001-11-01},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research Psychologische Forschung},
  volume = {65},
  pages = {235--241},
  issn = {1430-2772},
  doi = {10.1007/s004260100059},
  abstract = {It is hypothesized that eye movements are used to coordinate elements of a mental model with elements of the visual field. In two experiments, eye movements were recorded while observers imagined or recalled objects that were not present in the visual display. In both cases, observers spontaneously looked at particular blank regions of space in a systematic fashion, to manipulate and organize spatial relationships between mental and/or retinal images. These results contribute to evidence that interpreting a linguistic description of a visual scene requires a spatial (mental model) representation, and they support claims regarding the allocation of position markers in visual space for the manipulation of visual attention. More broadly, our results point to a concrete embodiment of cognition, in that a construction of a mental image is almost “acted out” by the eye movements, and a mental search of internal memory is accompanied by an ocolumotor search of external space.},
  file = {/Users/rachelryskin/Zotero/storage/4W3IHLCY/Spivey and Geng - 2001 - Oculomotor mechanisms activated by imagery and mem.pdf},
  keywords = {eye-tracking,memory},
  langid = {english},
  number = {4},
}
@Article{snedekerDevelopingConstraintsParsing2004a,
  title = {The Developing Constraints on Parsing Decisions: {{The}} Role of Lexical-Biases and Referential Scenes in Child and Adult Sentence Processing},
  shorttitle = {The Developing Constraints on Parsing Decisions},
  author = {Jesse Snedeker and John C. Trueswell},
  date = {2004-11},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {49},
  pages = {238--299},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2004.03.001},
  file = {/Users/rachelryskin/Zotero/storage/BUZAVHDA/Snedeker and Trueswell - 2004 - The developing constraints on parsing decisions T.pdf},
  langid = {english},
  number = {3},
}
@Manual{R-afex,
  title = {afex: Analysis of Factorial Experiments},
  author = {Henrik Singmann and Ben Bolker and Jake Westfall and Frederik Aust and Mattan S. Ben-Shachar},
  year = {2021},
  note = {R package version 0.28-1},
  url = {https://CRAN.R-project.org/package=afex},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2021},
  note = {R package version 1.0.7},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2021},
  note = {R package version 0.5.1},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Article{R-jsonlite,
  title = {The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects},
  author = {Jeroen Ooms},
  journal = {arXiv:1403.2805 [stat.CO]},
  year = {2014},
  url = {https://arxiv.org/abs/1403.2805},
}
@Article{R-lme4,
  title = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  journal = {Journal of Statistical Software},
  year = {2015},
  volume = {67},
  number = {1},
  pages = {1--48},
  doi = {10.18637/jss.v067.i01},
}
@Article{R-lmerTest,
  title = {{lmerTest} Package: Tests in Linear Mixed Effects Models},
  author = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B. Christensen},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {82},
  number = {13},
  pages = {1--26},
  doi = {10.18637/jss.v082.i13},
}
@Manual{R-Matrix,
  title = {Matrix: Sparse and Dense Matrix Classes and Methods},
  author = {Douglas Bates and Martin Maechler},
  year = {2021},
  note = {R package version 1.3-3},
  url = {https://CRAN.R-project.org/package=Matrix},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester},
  year = {2020},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2019},
  note = {R package version 1.4.0},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Manual{R-tidyr,
  title = {tidyr: Tidy Messy Data},
  author = {Hadley Wickham},
  year = {2021},
  note = {R package version 1.1.3},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Article{deleeuwJsPsychJavaScriptLibrary2015,
  title = {{{jsPsych}}: {{A JavaScript}} Library for Creating Behavioral Experiments in a {{Web}} Browser},
  shorttitle = {{{jsPsych}}},
  author = {Joshua R. {de Leeuw}},
  date = {2015-03-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {47},
  pages = {1--12},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0458-y},
  abstract = {Online experiments are growing in popularity, and the increasing sophistication of Web technology has made it possible to run complex behavioral experiments online using only a Web browser. Unlike with offline laboratory experiments, however, few tools exist to aid in the development of browser-based experiments. This makes the process of creating an experiment slow and challenging, particularly for researchers who lack a Web development background. This article introduces jsPsych, a JavaScript library for the development of Web-based experiments. jsPsych formalizes a way of describing experiments that is much simpler than writing the entire experiment from scratch. jsPsych then executes these descriptions automatically, handling the flow from one task to another. The jsPsych library is open-source and designed to be expanded by the research community. The project is available online at www.jspsych.org.},
  file = {/Users/rachelryskin/Zotero/storage/U5J3EEUM/de Leeuw - 2015 - jsPsych A JavaScript library for creating behavio.pdf},
  langid = {english},
  number = {1},
  options = {useprefix=true},
}
@InProceedings{papoutsaki2016webgazer,
  title = {{{WebGazer}}: {{Scalable}} Webcam Eye Tracking Using User Interactions},
  booktitle = {Proceedings of the 25th International Joint Conference on Artificial Intelligence ({{IJCAI}})},
  author = {Alexandra Papoutsaki and Patsorn Sangkloy and James Laskey and Nediyana Daskalova and Jeff Huang and James Hays},
  date = {2016},
  pages = {3839--3845},
  organization = {{AAAI}},
}
@Manual{R-shiny,
  title = {shiny: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Carson Sievert and Barret Schloerke and Yihui Xie and Jeff Allen and Jonathan McPherson and Alan Dipert and Barbara Borges},
  year = {2021},
  note = {R package version 1.6.0},
  url = {https://CRAN.R-project.org/package=shiny},
}
@Manual{R-broom.mixed,
  title = {broom.mixed: Tidying Methods for Mixed Models},
  author = {Ben Bolker and David Robinson},
  year = {2020},
  note = {R package version 0.2.6},
  url = {https://CRAN.R-project.org/package=broom.mixed},
}
@Manual{R-tinylabels,
  title = {{tinylabels}: Lightweight Variable Labels},
  author = {Marius Barth},
  year = {2022},
  note = {R package version 0.2.3},
  url = {https://cran.r-project.org/package=tinylabels},
}
@article{simonsohn2015,
author = {Uri Simonsohn},
title ={Small Telescopes: Detectability and the Evaluation of Replication Results},
journal = {Psychological Science},
volume = {26},
number = {5},
pages = {559-569},
year = {2015},
doi = {10.1177/0956797614567341},
    note ={PMID: 25800521},

URL = { 
        https://doi.org/10.1177/0956797614567341
    
},
eprint = { 
        https://doi.org/10.1177/0956797614567341
    
}
,
    abstract = { This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating “unsuccessful” replication attempts (i.e., studies yielding p > .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) “protecting” true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular. }
}
